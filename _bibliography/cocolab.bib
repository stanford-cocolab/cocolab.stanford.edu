%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Noah Goodman at 2015-07-30 09:00:45 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Luong2015,
	Author = {Thang Luong and Timothy O'Donnell and Noah D. Goodman},
	Booktitle = {CogACLL 2015},
	Date-Added = {2015-07-30 15:58:46 +0000},
	Date-Modified = {2015-07-30 16:00:44 +0000},
	Title = {Evaluating Models of Computation and Storage in Human Sentence Processing},
	Year = {2015}}

@article{OngEtAl2015-Cognition,
	Abstract = {Humans skillfully reason about others' emotions, a phenomenon we term affective cognition. Despite its importance, few formal, quantitative theories have described the mechanisms supporting this phenomenon. We propose that affective cognition involves applying domain-general reasoning processes to domain-specific content knowledge. Observers' knowledge about emotions is represented in rich and coherent lay theories, which comprise consistent relationships between situations, emotions, and behaviors. Observers utilize this knowledge in deciphering social agents' behavior and signals (e.g., facial expressions), in a manner similar to rational inference in other domains. We construct a computational model of a lay theory of emotion, drawing on tools from Bayesian statistics, and test this model across four experiments in which observers drew inferences about others' emotions in a simple gambling paradigm. This work makes two main contributions. First, the model accurately captures observers' flexible but consistent reasoning about the ways that events and others' emotional responses to those events relate to each other. Second, our work models the problem of emotional cue integration---reasoning about others' emotion from multiple emotional cues---as rational inference via Bayes' rule, and we show that this model tightly tracks human observers' empirical judgments. Our results reveal a deep structural relationship between affective cognition and other forms of inference, and suggest wide-ranging applications to basic psychological theory and psychiatry.},
	Author = {Desmond C. Ong and Jamil Zaki and Noah D. Goodman},
	Date-Modified = {2015-07-30 15:55:10 +0000},
	Journal = {Cognition},
	Title = {Affective Cognition: Exploring lay theories of emotion},
	Website = {papers/OngEtAl2015-Cognition.pdf},
	Year = {2015}}

@unpublished{BergenLevyGoodman2014,
	Abstract = {A number of recent proposals have used techniques from game theory and Bayesian cognitive science to formalize Gricean pragmatic reasoning (Frank & Goodman, 2012; Franke, 2009; Goodman & Stuhlm\"uller, 2013; Jaeger, 2012). We discuss two phenomena which pose a challenge to these accounts of pragmatics: M-implicatures (Horn, 1984) and embedded implicatures which violate Hurford's constraint (Chierchia, Fox & Spector, 2012; Hurford, 1974). Previous models cannot derive these implicatures, because of basic limitations in their architecture. In order to explain these phenomena, we propose a realignment of the division between semantic content and pragmatic content. Under this proposal, the semantic content of an utterance is not fixed independent of pragmatic inference; rather, pragmatic inference partially determines an utterance's semantic content. We show how semantic inference can be realized as an extension to the Rational Speech Acts framework (Goodman & Stuhlm\"uller, 2013). The addition of lexical uncertainty derives both M-implicatures and the relevant embedded implicatures, and preserves the derivations of more standard implicatures.},
	Annote = {(Unpublished manuscript.)},
	Author = {Leon Bergen and Roger Levy and Noah D Goodman},
	Title = {Pragmatic Reasoning through Semantic Inference},
	Website = {papers/BergenLevyGoodman2014.pdf},
	Year = {Manuscript}}

@inproceedings{SumnerEtAl2015-Cogsci,
	Abstract = {A popular conception about language development is that comprehension precedes production. Although this is certainly true during the earliest stages of phonological development, once a child possesses the basic articulatory skills required for imitation, it need not necessarily be the case. A child could produce a word without possessing the fully formed lexical representation through imitation. In some cases, such as in response to questions containing fixed choices, this behavior could be mistaken for a deeper understanding of the words' semantic content. In this paper, we present evidence that 2to 3-year-old children exhibit a robust recency bias when verbally responding to two-alternative choice questions (i.e., they select the second, most recently mentioned option), possibly due to the availability of the second word in phonological memory. We find further evidence of this effect outside of a laboratory setting in naturalistic conversational contexts in CHILDES (MacWhinney, 2000), a large corpus of transcribed child-adult interactions.},
	Author = {Emily Sumner and Erika DeAngelis and Mara Hyatt and Noah D. Goodman and Celeste Kidd},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:57:29 +0000},
	Title = {Toddlers Always Get the Last Word: Recency biases in early verbal behavior},
	Website = {papers/SumnerEtAl2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{RitchieEtAl2015-SIGGRAPH,
	Abstract = {We present a method for controlling the output of procedural modeling programs using Sequential Monte Carlo (SMC). Previous probabilistic methods for controlling procedural models use Markov Chain Monte Carlo (MCMC), which receives control feedback only for completely-generated models. In contrast, SMC receives feedback incrementally on incomplete models, allowing it to reallocate computational resources and converge quickly. To handle the many possible sequentializations of a structured, recursive procedural modeling program, we develop and prove the correctness of a new SMC variant, Stochastically-Ordered Sequential Monte Carlo (SOSMC). We implement SOSMC for general-purpose programs using a new programming primitive: the stochastic future. Finally, we show that SOSMC reliably generates high-quality outputs for a variety of programs and control scoring functions. For small computational budgets, SOSMC's outputs often score nearly twice as high as those of MCMC or normal SMC.},
	Author = {Daniel Ritchie and B. Mildenhall and N. D. Goodman and P. Hanrahan},
	Booktitle = {SIGGRAPH 2015},
	Date-Modified = {2015-07-30 15:55:23 +0000},
	Title = {Controlling Procedural Modeling Programs with Stochastically-Ordered Sequential Monte Carlo},
	Website = {papers/RitchieEtAl2015-SIGGRAPH.pdf},
	Year = {2015}}

@inproceedings{OngEtAl2015-Cogsci,
	Abstract = {Observers often judge agents who miss a desired outcome by a small, compared to a large, margin to be less happy. This near-miss effect has typically been examined in situations where the agents have control over outcomes (e.g., missing a flight). Here, we extend this work in three ways. First, we show that near-miss effects play into observers' intuitive theories of emotion even for randomly-determined outcomes over which agents demonstrably have no control. Second, we find data consistent with a hypothesis in which -- even in randomly determined cases -- near-miss effects reflect an illusion of control over those events. Finally, we integrate near-miss effects into a broader model of affective cognition, and quantify the psychological cost of a missing a desired outcome by relatively little distance, relative to winning or losing that outcome.},
	Author = {Desmond C. Ong and Noah D. Goodman and Jamil Zaki},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:56:29 +0000},
	Title = {Near-misses sting even when they are uncontrollable},
	Website = {papers/OngEtAl2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{KrafftEtAl2015-Cogsci,
	Abstract = {Despite its importance, human collective intelligence remains enigmatic. We know what features are predictive of collective intelligence in human groups, but we do not understand the specific mechanisms that lead to the emergence of this distributed information processing ability. In contrast, there is a well-developed literature of experiments that have exposed the mechanisms of collective intelligence in nonhuman animal species. We adapt a recent experiment designed to study collective sensing in groups of fish in order to better understand the mechanisms that may underly the emergence of collective intelligence in human groups. We find that humans in our experiments act at a high level like fish but with two additional behaviors: independent exploration and targeted copying. These distinctively human activities may partially explain the emergence of collective sensing in our task environment at group sizes and on times scales orders of magnitudes smaller than were observed in fish.},
	Annote = {<b>[Winner of the 2015 Cognitive Science Society computational modeling prize for Applied Cognition.]</b>},
	Author = {Peter M. Krafft and Robert X. D. Hawkins and Alex Pentland and Noah D. Goodman and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:55:33 +0000},
	Title = {Emergent Collective Sensing in Human Groups},
	Website = {papers/KrafftEtAl2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{KaoEtAl2015-Cogsci,
	Abstract = {Verbal irony plays an important role in how we communicate and express opinions about the world. While there exist many theories and empirical findings about how people use and understand verbal irony, there is to our knowledge no formal model of how people incorporate shared background knowledge and linguistic information to communicate ironically. Here we argue that a computational approach previously shown to model hyperbole (Kao, Wu, Bergen, & Goodman, 2014) can also explain irony once we extend it to a richer space of affective subtext. We then describe two behavioral experiments that examine peoples interpretations of utterances in contexts that afford irony. We show that by minimally extending the hyperbole model to account for two dimensions of affect -- valence and arousal -- our model produces interpretations that closely match humans'. We discuss the implications of our model on informal theories of irony and its relationship to other types of nonliteral language understanding.},
	Author = {Justine T Kao and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:56:19 +0000},
	Title = {Let's talk (ironically) about the weather: Modeling verbal irony},
	Website = {papers/KaoEtAl2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{IcardGoodman2015-Cogsci,
	Abstract = {The causal frame problem is an epistemological puzzle about how the mind is able to disregard seemingly irrelevant causal knowledge, and focus on those factors that promise to be useful in making an inference or coming to a decision. Taking a subject's causal knowledge to be (implicitly) represented in terms of directed graphical models, the causal frame problem can be construed as the question of how to determine a reasonable "submodel" of one's "full model" of the world, so as to optimize the balance between accuracy in prediction on the one hand, and computational costs on the other. We propose a framework for addressing this problem, and provide several illustrative examples based on HMMs and Bayes nets. We also show that our framework can account for some of the recent empirical phenomena associated with alternative neglect.},
	Author = {Icard III, Thomas Frederick and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:54:55 +0000},
	Title = {A Resource-Rational Approach to the Causal Frame Problem},
	Website = {papers/IcardGoodman2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{HawthorneGoodman2015-Cogsci,
	Abstract = {In standard decision theory, rational agents are objective, keeping their beliefs independent from their desires (Berger, 1985). Such agents are the basis for current computational models of Theory of Mind (ToM), but this fundamental assumption of the theory remains untested. Do people think that others' beliefs are objective, or do they think that others' desires color their beliefs? We describe a Bayesian framework for exploring this relationship and its implications. Motivated by this analysis, we conducted two experiments testing the a priori independence of beliefs and desires in people's ToM and find that, contrary to fully-normative accounts, people think that others engage in wishful thinking. In the first experiment, we found that people think others believe both that desirable events are more likely to happen, and that undesirable ones are less likely to happen. In the second experiment, we found that social learning leverages this intuitive understanding of wishful thinking: participants learned more from the beliefs of an informant whose desires were contrary to his beliefs. People's ToM therefore appears to be more nuanced than the current rational accounts, but consistent with a model in which desire directly affects the subjective probability of an event.},
	Author = {Daniel Hawthorne and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:57:11 +0000},
	Title = {So good it has to be true: Wishful thinking in theory of mind},
	Website = {papers/HawthorneGoodman2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{HawkinsEtAl2015-Cogsci,
	Abstract = {What makes a question useful? What makes an answer appropriate? In this paper, we formulate a family of increasingly sophisticated models of question-answer behavior within the Rational Speech Act framework. We compare these models based on three different pieces of evidence: first, we demonstrate how our answerer models capture a classic effect in psycholinguistics showing that an answerer's level of informativeness varies with the inferred questioner goal, while keeping the question constant. Second, we jointly test the questioner and answerer components of our model based on empirical evidence from a question-answer reasoning game. Third, we examine a special case of this game to further distinguish among the questioner models. We find that sophisticated pragmatic reasoning is needed to account for some of the data. People can use questions to provide cues to the answerer about their interest, and can select answers that are informative about inferred interests.},
	Author = {Robert X. D. Hawkins and Andreas Stuhlm\"uller and Judith Degen and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:57:43 +0000},
	Title = {Why do you ask? Good questions provoke informative answers},
	Website = {papers/HawkinsEtAl2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{GerstenbergEtAl2015-Cogsci,
	Abstract = {How do people make causal judgments? Here, we propose a counterfactual simulation model (CSM) of causal judgment that unifies different views on causation. The CSM predicts that people's causal judgments are influenced by whether a candidate cause made a difference to whether the outcome occurred as well as to how it occurred. We show how whethercausation and how-causation can be implemented in terms of different counterfactual contrasts defined over the same intuitive generative model of the domain. We test the model in an intuitive physics domain where people make judgments about colliding billiard balls. Experiment 1 shows that participants' counterfactual judgments about what would have happened if one of the balls had been removed, are well-explained by an approximately Newtonian model of physics. In Experiment 2, participants judged to what extent two balls were causally responsible for a third ball going through a gate or missing the gate. As predicted by the CSM, participants' judgments increased with their belief that a ball was a whether-cause, a how-cause, as well as sufficient for bringing about the outcome.},
	Author = {Gerstenberg, Tobias and Noah D. Goodman and Lagnado, David A and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:56:09 +0000},
	Title = {How, whether, why: Causal judgments as counterfactual contrasts},
	Website = {papers/GerstenbergEtAl2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{DegenEtAl2015-Cogsci,
	Abstract = {World knowledge enters into pragmatic utterance interpretation in complex ways, and may be defeasible in light of speakers' utterances. Yet there is to date a surprising lack of systematic investigation into the role of world knowledge in pragmatic inference. In this paper, we show that a state-of-the-art model of pragmatic interpretation greatly overestimates the influence of world knowledge on the interpretation of utterances like Some of the marbles sank. We extend the model to capture the idea that the listener is uncertain about the background knowledge the speaker is bringing to the conversation. This extension greatly improves model predictions of listeners' interpretation and also makes good qualitative predictions about listeners' judgments of how 'normal' the world is in light of a speaker's statement. Theoretical and methodological implications are discussed.},
	Author = {Judith Degen and Michael Henry Tessler and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:58:02 +0000},
	Title = {Wonky worlds: Listeners revise world knowledge when utterances are odd},
	Website = {papers/DegenEtAl2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{BennettGoodman2015-Cogsci,
	Abstract = {We show how the wide range in strengths of intensifying degree adverbs (e.g. very and extremely) could be explained by pragmatic inference based on differing cost, rather than differing semantics. This predicts a linear relationship between the meaning of intensifiers and their length and log-frequency. We test this prediction in two studies, using two different dependent measures, finding that higher cost does predict stronger meanings. We discuss the implications for adverbial meaning and the more general question of how extensive non-arbitrary form-meaning association may be in language.},
	Author = {Erin Bennett and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:55:54 +0000},
	Title = {Extremely costly intensifiers are stronger than quite costly ones},
	Website = {papers/BennettGoodman2015-Cogsci.pdf},
	Year = {2015}}

@inproceedings{BassEtAl2015-Cogsci,
	Abstract = {What constitutes good teaching, and what factors do learners consider when evaluating teachers? Prior developmental work suggests that even young children accurately recognize and evaluate under-informativeness. Building on prior work, we propose a Bayesian model of teacher evaluation that infers the teacher's quality from how carefully he selected demonstrations given what he knew. We test the predictions of our model across 15 conditions in which participants saw a teacher who demonstrated all or a subset of functions of a novel device and rated his helpfulness. Our results suggest that human adults seamlessly integrate information about the number of functions taught, their values, as well as what the teacher knew, to make nuanced judgments about the quality of teaching; the quantitative pattern is well predicted by our model.},
	Author = {Ilona Bass and Daniel Hawthorne and Noah D. Goodman and Hyowon Gweon},
	Booktitle = {Proceedings of the Thirty-Seventh Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Modified = {2015-07-30 15:56:39 +0000},
	Title = {Not by number alone: The effect of teacher's knowledge and its value in evaluating "sins of omission"},
	Website = {papers/BassEtAl2015-Cogsci.pdf},
	Year = {2015}}

@article{LassiterGoodman2015-Synthese,
	Abstract = {We derive a probabilistic account of the vagueness and context-sensitivity of scalar adjectives from a Bayesian approach to communication and interpretation. We describe an iterated reasoning architecture for pragmatic interpretation and illustrate it with a simple scalar implicature example. We then show how to enrich the apparatus to handle pragmatic reasoning about the values of free variables, explore its predictions about the interpretation of scalar adjectives, and show how this model implements Edgington's (1992; 1997) account of the sorites paradox, with variations. The Bayesian approach has a number of explanatory virtues: in particular, it does not require any specialpurpose machinery for handling vagueness, and it is integrated with a promising new approach to pragmatics and other areas of cognitive science.},
	Author = {D. Lassiter and N. D. Goodman},
	Date-Modified = {2015-06-28 14:22:52 +0000},
	Journal = {Synthese},
	Title = {Adjectival vagueness in a Bayesian model of interpretation},
	Website = {papers/LassiterGoodman2015-Synthese.pdf},
	Year = {2015}}

@article{KaoEtAl2015-CognitiveScience,
	Abstract = {Humor plays an essential role in human interactions. Precisely what makes something funny, however, remains elusive. While research on natural language understanding has made significant advancements in recent years, there has been little direct integration of humor research with computational models of language understanding. In this paper, we propose two information-theoretic measures---ambiguity and distinctiveness---derived from a simple model of sentence processing. We test these measures on a set of puns and regular sentences and show that they correlate significantly with human judgments of funniness. Moreover, within a set of puns, the distinctiveness measure distinguishes exceptionally funny puns from mediocre ones. Our work is the first, to our knowledge, to integrate a model of general language understanding and humor theory to quantitatively predict humor at a fine-grained level. We present it as an example of a framework for applying models of language processing to understand higher-level linguistic and cognitive phenomena.},
	Author = {Justine T Kao and Roger Levy and Noah D Goodman},
	Date-Modified = {2015-07-30 15:54:33 +0000},
	Journal = {Cognitive Science},
	Title = {A computational model of linguistic humor in puns},
	Website = {papers/KaoEtAl2015-CognitiveScience.pdf},
	Year = {2015}}

@article{GriffithsEtAl2015-TiCS,
	Abstract = {Marr's levels of analysis -- computational, algorithmic, and implementation -- have served cognitive science well over the last 30 years. But the recent increase in the popularity of the computational level raises a new challenge: How do we begin to relate models at different levels of analysis? We propose that it is possible to define levels of analysis that lie between the computational and the algorithmic, providing a way to build a bridge between computational- and algorithmic-level models. The key idea is to push the notion of rationality, often used in defining computational-level models, deeper toward the algorithmic level. We offer a simple recipe for reverse-engineering the mind's cognitive strategies by deriving optimal algorithms for a series of increasingly more realistic abstract computational architectures, which we call "resource-rational analysis."},
	Annote = {(to appear)},
	Author = {T. L. Griffiths and F. Lieder and N. D. Goodman},
	Journal = {Topics in Cognitive Science},
	Title = {Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic},
	Website = {papers/GriffithsEtAl2015-TiCS.pdf},
	Year = {To appear}}

@article{BergenGoodman2015-TiCS,
	Abstract = {We combine two recent probabilistic approaches to natural language understanding, exploring the formal pragmatics of communication on a noisy channel. We first extend a model of rational communication between a speaker and listener, to allow for the possibility that messages are corrupted by noise. In this model, common knowledge of a noisy channel leads to the use and correct understanding of sentence fragments. A further extension of the model, which allows the speaker to intentionally reduce the noise rate on a word, is used to model prosodic emphasis. We show that the model derives several well-known changes in meaning associated with prosodic emphasis. Our results show that nominal amounts of actual noise can be leveraged for communicative purposes.},
	Annote = {(to appear)},
	Author = {L. Bergen and N. D. Goodman},
	Journal = {Topics in Cognitive Science},
	Title = {The strategic use of noise in pragmatic reasoning},
	Website = {papers/BergenGoodman2015-TiCS.pdf},
	Year = {To appear}}

@inproceedings{RitchieEtAl2015-Eurographics,
	Abstract = {We present a system for generating suggestions from highly-constrained, continuous design spaces. We formulate suggestion as sampling from a probability distribution; constraints are represented as factors that concentrate probability mass around sub-manifolds of the design space. These sampling problems are intractable using typical random walk MCMC techniques, so we adopt Hamiltonian Monte Carlo (HMC), a gradient-based MCMC method. We implement HMC in a high-performance probabilistic programming language, and we evaluate its ability to efficiently generate suggestions for two different, highly-constrained example applications: vector art coloring and designing stable stacking structures.},
	Annote = {<b>[Best paper award honorable mention.]</b>},
	Author = {Daniel Ritchie and Sharon Lin and Noah D. Goodman and Pat Hanrahan},
	Booktitle = {Proceedings of Eurographics 2015},
	Title = {{Generating Design Suggestions under Tight Constraints with Gradient-based Probabilistic Programming}},
	Website = {papers/RitchieEtAl2015-Eurographics.pdf},
	Year = {2015}}

@article{LassiterGoodman2015-Cognition,
	Abstract = {The "new paradigm" unifying deductive and inductive reasoning in a Bayesian framework (Oaksford & Chater, 2007; Over, 2009) has been claimed to be falsified by results which show sharp differences between reasoning about necessity vs. plausibility (Heit & Rotello, 2010; Rips, 2001; Rotello & Heit, 2009). We provide a probabilistic model of reasoning with modal expressions such as "necessary" and "plausible" informed by recent work in formal semantics of natural language, and show that it predicts the possibility of non-linear response patterns which have been claimed to be problematic. Our model also makes a strong monotonicity prediction, while two-dimensional theories predict the possibility of reversals in argument strength depending on the modal word chosen. Predictions were tested using a novel experimental paradigm that replicates the previously-reported response patterns with a minimal manipulation, changing only one word of the stimulus between conditions. We found a spectrum of reasoning "modes" corresponding to different modal words, and strong support for our model's monotonicity prediction. This indicates that probabilistic approaches to reasoning can account in a clear and parsimonious way for data previously argued to falsify them, as well as new, more fine-grained, data. It also illustrates the importance of careful attention to the semantics of language employed in reasoning experiments.},
	Author = {D. Lassiter and N. D. Goodman},
	Journal = {Cognition},
	Title = {How many kinds of reasoning? Inference, probability, and natural language semantics},
	Website = {papers/LassiterGoodman2015-Cognition.pdf},
	Year = {2015}}

@inbook{GoodmanEtAl2015-Chapter,
	Abstract = {Knowledge organizes our understanding of the world, determining what we expect given what we have already seen. Our predictive representations have two key properties: they are productive, and they are graded. Productive generalization is possible because our knowledge decomposes into concepts -- elements of knowledge that are combined and recombined to describe particular situations. Gradedness is the observable effect of accounting for uncertainty -- our knowledge encodes degrees of belief that lead to graded probabilistic predictions. To put this a different way, concepts form a combinatorial system that enables description of many different situations; each such situation specifies a distribution over what we expect to see in the world, given what we have seen. We may think of this system as a probabilistic language of thought (PLoT) in which representations are built from language-like composition of concepts and the content of those representations is a probability distribution on world states. The purpose of this chapter is to formalize these ideas in computational terms, to illustrate key properties of the PLoT approach with a concrete example, and to draw connections with other views of conceptual structure.},
	Author = {Goodman, Noah D and Tenenbaum, Joshua B and Gerstenberg, T},
	Booktitle = {The Conceptual Mind: New Directions in the Study of Concepts},
	Editor = {Morgolis and Lawrence},
	Publisher = {MIT Press},
	Title = {Concepts in a probabilistic language of thought},
	Website = {papers/GoodmanEtAl2015-Chapter.pdf},
	Year = {2015}}

@inbook{GoodmanLassiter2015-Chapter,
	Abstract = {Language is used to communicate ideas. Ideas are mental tools for coping with a complex and uncertain world. Thus human conceptual structures should be key to language meaning, and probability -- the mathematics of uncertainty -- should be indispensable for describing both language and thought. Indeed, probabilistic models are enormously useful in modeling human cognition (Tenenbaum et al., 2011) and aspects of natural language (Bod et al., 2003; Chater et al., 2006). With a few early exceptions (e.g. Adams, 1975; Cohen, 1999b), probabilistic tools have only recently been used in natural language semantics and pragmatics. In this chapter we synthesize several of these modeling advances, exploring a formal model of interpretation grounded, via lexical semantics and pragmatic inference, in conceptual structure.},
	Author = {Noah D. Goodman and Daniel Lassiter},
	Booktitle = {The Handbook of Contemporary Semantic Theory, 2nd Edition},
	Editor = {Shalom Lappin and Chris Fox},
	Publisher = {Wiley-Blackwell},
	Title = {Probabilistic Semantics and Pragmatics: Uncertainty in Language and Thought},
	Website = {papers/GoodmanLassiter2015-Chapter.pdf},
	Year = {2015}}

@article{GoodmanEtAl2015-PsychSci,
	Abstract = {Computational models in psychology are precise, fully explicit scientific hypotheses. Over the past 15 years, probabilistic modeling of human cognition has yielded quantitative theories of a wide variety of reasoning and learning phenomena. Recently, Marcus and Davis (2013) critique several examples of this work, using these critiques to question the basic validity of the probabilistic approach. Contra the broad rhetoric of their article, the points made by Marcus and Davis -- while useful to consider -- do not indicate systematic problems with the probabilistic modeling enterprise.},
	Author = {N. D. Goodman and M. C. Frank and T. L. Griffiths and J. B. Tenenbaum and P. Battaglia and J. Hamrick},
	Journal = {Psychological Science},
	Title = {Relevant and robust. A response to Marcus and Davis},
	Website = {papers/GoodmanEtAl2015-PsychSci.pdf},
	Year = {2015}}

@article{PiersonGoodman2014-PLoSONE,
	Abstract = {Classical decision theory predicts that people should be indifferent to information that is not useful for making decisions, but this model often fails to describe human behavior. Here we investigate one such scenario, where people desire information about whether an event (the gain/loss of money) will occur even though there is no obvious decision to be made on the basis of this information. We find a curious dual trend: if information is costless, as the probability of the event increases people want the information more; if information is not costless, people's desire for the information peaks at an intermediate probability. People also want information more as the importance of the event increases, and less as the cost of the information increases. We propose a model that explains these results, based on the assumption that people have limited cognitive resources and obtain information about which events will occur so they can determine whether to expend effort planning for them.},
	Author = {E. Pierson and N. D. Goodman},
	Journal = {PLoS ONE},
	Title = {Uncertainty and denial: a resource-rational model of the value of information},
	Website = {papers/PiersonGoodman2014-PLoSONE.pdf},
	Year = {2014}}

@article{KaoEtAl2014-PNAS,
	Abstract = {One of the most puzzling and important facts about communication is that people do not always mean what they say; speakers often use imprecise, exaggerated, or otherwise literally false descriptions to communicate experiences and attitudes. Here, we focus on the nonliteral interpretation of number words, in particular hyperbole (interpreting unlikely numbers as exaggerated and conveying affect) and pragmatic halo (interpreting round numbers imprecisely). We provide a computational model of number interpretation as social inference regarding the communicative goal, meaning, and affective subtext of an utterance. We show that our model predicts humans' interpretation of number words with high accuracy. Our model is the first to our knowledge to incorporate principles of communication and empirically measured background knowledge to quantitatively predict hyperbolic and pragmatic halo effects in number interpretation. This modeling framework provides a unified approach to nonliteral language understanding more generally.},
	Author = {Justine T Kao and Jean Wu and Leon Bergen and Noah D Goodman},
	Date-Added = {2014-07-02 14:29:32 +0000},
	Date-Modified = {2014-08-26 20:27:55 +0000},
	Journal = {Proceedings of the {N}ational {A}cademy of {S}ciences},
	Title = {Nonliteral understanding of number words},
	Website = {papers/KaoEtAl2014-PNAS.pdf},
	Year = {2014}}

@article{StillerEtAl2014,
	Abstract = {If a speaker tells us that "some guests were late to the party," we typically infer that not all were. Implicatures, in which an ambiguous statement ("some and possibly all") is strengthened pragmatically (to "some and not all"), are a paradigm case of pragmatic reasoning. Inferences of this sort are difficult for young children, but recent work suggests that this mismatch may stem from issues in understanding the relationship between lexical items like "some" and "all," rather than broader pragmatic deficits. We tested children's ability to make non-quantificational pragmatic inferences by constructing contextually-derived "ad-hoc" implicatures, using sets of pictures with contrasting features. We found that four-year-olds and some three-year-olds were able to make implicatures successfully using these displays. Hence, apparent failures in scalar implicature are likely due to difficulties specific to the constructions and tasks used in previous work; these difficulties may have masked aspects of children's underlying pragmatic competence.},
	Author = {Stiller, A. J. and Goodman, N. D. and Frank, M. C.},
	Date-Modified = {2014-07-01 15:38:39 +0000},
	Journal = {Language Learning and Development},
	Title = {Ad-hoc scalar implicature in preschool children},
	Website = {https://langcog.stanford.edu/papers/SGF-lldinpress.pdf},
	Year = {2014}}

@inproceedings{YangEtAl2014,
	Abstract = {Universal probabilistic programming languages (such as Church) trade performance for abstraction: any model can be represented compactly as an arbitrary stochastic computation, but costly online analyses are required for inference. We present a technique that recovers hand-coded levels of performance from a universal probabilistic language, for the Metropolis-Hastings (MH) MCMC inference algorithm. It takes a Church program as input and traces its execution to remove computation overhead. It then analyzes the trace for each proposal, using slicing, to identify the minimal computation needed to evaluate the MH acceptance probability. Generated incremental code is much faster than a baseline implementation (up to 600x) and usually as fast as handcoded MH kernels.},
	Author = {Yang, L. and Hanrahan, P., and Goodman, N. D.},
	Booktitle = {AISTATS},
	Title = {Generating Efficient MCMC Kernels from Probabilistic Programs},
	Website = {https://web.stanford.edu/~ngoodman/papers/aistats2014-shred.pdf},
	Year = {2014}}

@article{ShaftoEtAl2014,
	Abstract = {Much of learning and reasoning occurs in pedagogical situations -- situations in which a person who knows a concept chooses examples for the purpose of helping a learner acquire the concept. We introduce a model of teaching and learning in pedagogical settings that predicts which examples teachers should choose and what learners should infer given a teacher's examples. We present three experiments testing the model predictions for rule-based, prototype, and causally structured concepts. The model shows good quantitative and qualitative fits to the data across all three experiments, predicting novel qualitative phenomena in each case. We conclude by discussing implications for understanding concept learning and implications for theoretical claims about the role of pedagogy in human learning.},
	Author = {Patrick Shafto and Noah D Goodman and Thomas L. Griffiths},
	Journal = {Cognitive Psychology},
	Title = {A rational account of pedagogical reasoning: Teaching by, and learning from, examples},
	Website = {https://web.stanford.edu/~ngoodman/papers/shaftogg14.pdf},
	Year = {2014}}

@inproceedings{Tessler:2014wu,
	Abstract = {Syllogistic reasoning lies at the intriguing intersection of natural and formal reasoning, of language and logic. Syllogisms comprise a formal system of reasoning yet use natural language quantifiers, and invite natural language conclusions. How can we make sense of the interplay between logic and language? We develop a computational-level theory that considers reasoning over concrete situations, constructed probabilistically by sampling. The base model can be enriched to consider the pragmatics of natural language arguments. The model predictions are compared with behavioral data from a recent meta-analysis. The flexibility of the model is then explored in a data set of syllogisms using the generalized quantifiers most and few. We conclude by relating our model to two extant theories of syllogistic reasoning -- Mental Models and Probability Heuristics.},
	Author = {Tessler, Michael Henry and Goodman, Noah D.},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Some arguments are probably valid: Syllogistic reasoning as communication},
	Website = {https://web.stanford.edu/~ngoodman/papers/cogsci14-syllogisms_tessler.pdf},
	Year = {2014}}

@inproceedings{GerstenbergEtAl2014-Cogsci,
	Abstract = {In this paper, we demonstrate that people's causal judgments are inextricably linked to counterfactuals. In our experiments, participants judge whether one billiard ball A caused another ball B to go through a gate. Our counterfactual simulation model predicts that people arrive at their causal judgments by comparing what actually happened with the result of mentally simulating what would have happened in the relevant counterfactual world. We test our model against actualist theories of causation which aim to explain causation just in terms of what actually happened. Our experimental stimuli contrast cases in which we hold constant what actually happened but vary the counterfactual outcome. In support of our model, we find that participants' causal judgments differ drastically between such cases. People's cause and prevention judgments increase with their subjective degree of belief that the counterfactual outcome would have been different from what actually happened.},
	Author = {Gerstenberg, Tobias and Noah D. Goodman and Lagnado, David A and Tenenbaum, Joshua B},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {From counterfactual simulation to causal judgment},
	Website = {papers/GerstenbergEtAl2014-Cogsci.pdf},
	Year = {2014}}

@inproceedings{Gershman:2014wt,
	Abstract = {Recent studies of probabilistic reasoning have postulated general-purpose inference algorithms that can be used to answer arbitrary queries. These algorithms are memoryless, in the sense that each query is processed independently, without reuse of earlier computation. We argue that the brain operates in the setting of amortized inference, where numerous related queries must be answered (e.g., recognizing a scene from multiple viewpoints); in this setting, memoryless algorithms can be computationally wasteful. We propose a simple form of flexible reuse, according to which shared inferences are cached and composed together to answer new queries. We present experimental evidence that humans exploit this form of reuse: the answer to a complex query can be systematically predicted from a person's response to a simpler query if the simpler query was presented first and entails a sub-inference (i.e., a sub-component of the more complex query). People are also faster at answering a complex query when it is preceded by a sub-inference. Our results suggest that the astonishing efficiency of human probabilistic reasoning may be supported by interactions between inference and memory.},
	Author = {Sam Gershman and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-07 15:38:04 +0000},
	Date-Modified = {2014-04-07 15:38:45 +0000},
	Title = {Amortized inference in probabilistic reasoning},
	Website = {https://web.stanford.edu/~ngoodman/papers/amortized_inference.pdf},
	Year = {2014}}

@inproceedings{DegenGoodman2014-Cogsci,
	Abstract = {A rarely discussed but important issue in research on pragmatic inference is the choice of dependent measure for estimating the robustness of pragmatic inferences and their sensitivity to contextual manipulations. Here we present the results from three studies exploring the effect of contextual manipulations on scalar implicature. In all three studies we manipulate the salient question under discussion and the perceptual availability of relevant set sizes. The studies differ only in the dependent measure used: Exp. 1 uses truth judgements, Exp. 2 uses word probability ratings, and Exp. 3 uses a direct measure of sentence interpretation. We argue that the first two are effectively measures of production, and find they are sensitive to our contextual manipulations. In contrast the interpretation measure shows no effect of context. We argue that this methodologically troubling finding can be understood and predicted by using the framework of probabilistic pragmatics.},
	Author = {Judith Degen and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Lost your marbles? The puzzle of dependent measures in experimental pragmatics},
	Website = {papers/DegenGoodman2014-Cogsci.pdf},
	Year = {2014}}

@inproceedings{Bergen2014,
	Abstract = {We combine two recent probabilistic approaches to natural language understanding, exploring the formal pragmatics of communication on a noisy channel. We show that nominal amounts of actual noise can be leveraged for communicative purposes. Common knowledge of a noisy channel leads to the use and correct understanding of sentence fragments. Prosodic emphasis, interpreted as an intentional action to reduce noise on a word, results in strengthened meanings.},
	Annote = {<b>[Winner of the 2014 Cognitive Science Society computational modeling prize for Language.]</b>},
	Author = {Bergen, Leon and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-04 15:59:55 +0000},
	Date-Modified = {2014-04-07 15:34:52 +0000},
	Title = {The strategic use of noise in pragmatic reasoning},
	Website = {//web.mit.edu/bergen/www/papers/BergenGoodman2014.pdf},
	Year = {2014}}

@inproceedings{KaoEtAl2014-Cogsci,
	Abstract = {While the ubiquity and importance of nonliteral language are clear, people's ability to use and understand it remains a mystery. Metaphor in particular has been studied extensively across many disciplines in cognitive science. One approach focuses on the pragmatic principles that listeners utilize to infer meaning from metaphorical utterances. While this approach has generated a number of insights about how people understand metaphor, to our knowledge there is no formal model showing that effects in metaphor understanding can arise from basic principles of communication. Building upon recent advances in formal models of pragmatics, we describe a computational model that uses pragmatic reasoning to interpret metaphorical utterances. We conduct behavioral experiments to evaluate the model's performance and show that our model produces metaphorical interpretations that closely fit behavioral data. We discuss implications of the model for metaphor understanding, principles of communication, and formal models of language understanding},
	Author = {Kao, Justine T and Bergen, Leon and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirty-Sixth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2014-04-04 15:57:18 +0000},
	Date-Modified = {2014-04-07 15:34:44 +0000},
	Title = {Formalizing the pragmatics of metaphor understanding},
	Website = {papers/KaoEtAl2014-Cogsci.pdf},
	Year = {2014}}

@book{dippl,
	Annote = {(<code>http://dippl.org</code>)},
	Author = {Goodman, N. D. and Stuhlm\"uller, A.},
	Title = {The Design and Implementation of Probabilistic Programming Languages},
	Website = {http://dippl.org}}

@book{probmods,
	Abstract = {In this book, we explore the probabilistic approach to cognitive science, which models learning and reasoning as inference in complex probabilistic models. In particular, we examine how a broad range of empirical phenomena in cognitive science (including intuitive physics, concept learning, causal reasoning, social cognition, and language understanding) can be modeled using a functional probabilistic programming language called Church.},
	Annote = {(<code>https://probmods.org</code>)},
	Author = {Goodman, N. D. and Tenenbaum, J. B.},
	Date-Added = {2014-03-26 15:51:38 +0000},
	Date-Modified = {2014-03-26 15:53:06 +0000},
	Title = {Probabilistic Models of Cognition},
	Website = {https://probmods.org},
	Year = {2014}}

@unpublished{GoodmanGrant2013,
	Abstract = {Words are potentially one of the clearest windows on human knowledge and conceptual structure. But what do words mean? In this project we aim to construct and explore a formal model of lexical semantics grounded, via pragmatic inference, in core conceptual structures . Flexible human cognition is derived in large part from our ability to imagine possible worlds. A rich set of concepts, intuitive theories, and other mental representations support imagining and reasoning about possible worlds -- together we call these core cognition. Here we posit that the collection of core concepts also forms the set of primitive elements available for lexical semantics: word meanings are built from pieces of core cognition. We propose to study lexical semantics in the setting of an architecture for language understanding that integrates literal meaning with pragmatic inference. This architecture supports underspecified and uncertain lexical meaning, leading to subtle interactions between meaning, conceptual structure, and context. We will explore several cases of lexical semantics where these interactions are particularly important: indexicals, scalar adjectives, generics, and modals. We formalize both core cognition and the natural language architecture using the Church probabilistic programming language. In this project we aim to contribute to our understanding of the connection between words and mental representations; from this we expect to gain critical insights into many aspects of psychology, to construct vastly more useful thinking machines, and to interface natural and artificial intelligences more efficiently.},
	Annote = {(Unpublished manuscript.)},
	Author = {Goodman, N. D.},
	Title = {Grounding Lexical Meaning in Core Cognition},
	Website = {https://web.stanford.edu/~ngoodman/papers/LexSemSquibb.pdf},
	Year = {2013}}

@article{FrankGoodmanICOM,
	Abstract = {Language comprehension is more than a process of decoding the literal meaning of a speaker's utterance. Instead, by making the assumption that speakers choose their words to be informative in context, listeners routinely make pragmatic inferences that go beyond the linguistic data. If language learners make these same assumptions, they should be able to infer word meanings in otherwise ambiguous situations. We use probabilistic tools to formalize these kinds of informativeness inferences -- extending a model of pragmatic language comprehension to the acquisition setting -- and present four experiments whose data suggest that preschool children can use informativeness to infer word meanings and that adult judgments track quantitatively with informativeness.},
	Author = {M. C. Frank and N. D. Goodman},
	Date-Added = {2012-01-29 18:09:51 -0800},
	Date-Modified = {2014-07-02 14:41:49 +0000},
	Journal = {Cognitive Psychology},
	Title = {Inferring word meanings by assuming that speakers are informative},
	Website = {https://langcog.stanford.edu/papers/FG-cogpsych2014.pdf},
	Year = {2014}}

@article{Vul2014,
	Abstract = {In many learning or inference tasks human behavior approximates that of a Bayesian ideal observer, suggesting that, at some level, cognition can be described as Bayesian inference. However, a number of findings have highlighted an intriguing mismatch between human behavior and standard assumptions about optimality: People often appear to make decisions based on just one or a few samples from the appropriate posterior probability distribution, rather than using the full distribution. Although sampling-based approximations are a common way to implement Bayesian inference, the very limited numbers of samples often used by humans seem insufficient to approximate the required probability distributions very accurately. Here, we consider this discrepancy in the broader framework of statistical decision theory, and ask: If people are making decisions based on samples --  but as samples are costly -- how many samples should people use to optimize their total expected or worst-case reward over a large number of decisions? We find that under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods. These results help to reconcile a large body of work showing sampling-based or probability matching behavior with the hypothesis that human cognition can be understood in Bayesian terms, and they suggest promising future directions for studies of resource-constrained cognition.},
	Author = {E. Vul and N. D. Goodman and T. L. Griffiths and J. B. Tenenbaum},
	Journal = {Cognitive Science},
	Title = {One and Done? Optimal Decisions From Very Few Samples},
	Website = {https://web.stanford.edu/~ngoodman/papers/VulGoodmanGriffithsTenenbaum-COGS-2014.pdf},
	Year = {2014}}

@inproceedings{stuhlmuller2013learning,
	Abstract = {We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model's joint distribution: a factorization that turns observations into root nodes. Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly find a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples. To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler. We explore the efficiency of this sampler for a variety of parameter regimes and Bayes nets.},
	Author = {Stuhlm{\"u}ller, Andreas and Taylor, Jacob and Goodman, Noah},
	Booktitle = {Advances in Neural Information Processing Systems},
	Title = {Learning Stochastic Inverses},
	Website = {https://web.stanford.edu/~ngoodman/papers/inverses-nips-2013.pdf},
	Year = {2013}}

@inproceedings{smith2013learning,
	Abstract = {Language users are remarkably good at making inferences about speakers' intentions in context, and children learning their native language also display substantial skill in acquiring the meanings of unknown words. These two cases are deeply related: Language users invent new terms in conversation, and language learners learn the literal meanings of words based on their pragmatic inferences about how those words are used. While pragmatic inference and word learning have both been independently characterized in probabilistic terms, no current work unifies these two. We describe a model in which language learners assume that they jointly approximate a shared, external lexicon and reason recursively about the goals of others in using this lexicon. This model captures phenomena in word learning and pragmatic inference; it additionally leads to insights about the emergence of communicative systems in conversation and the mechanisms by which pragmatic inferences become incorporated into word meanings.},
	Author = {Smith, Nathaniel J and Goodman, Noah and Frank, Michael},
	Booktitle = {Advances in Neural Information Processing Systems 26},
	Editor = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
	Pages = {3039--3047},
	Publisher = {Curran Associates, Inc.},
	Title = {Learning and using language via recursive pragmatic reasoning about other agents},
	Website = {https://papers.nips.cc/paper/4929-learning-and-using-language-via-recursive-pragmatic-reasoning-about-other-agents.pdf},
	Year = {2013}}

@inproceedings{kaofunny,
	Abstract = {What makes something funny? Humor theorists posit that incongruity -- perceiving a situation from different viewpoints and finding the resulting interpretations to be incompatible -- contributes to sensations of mirth. In this paper, we use a computational model of sentence comprehension to formalize incongruity and test its relationship to humor in puns. By combining a noisy channel model of language comprehension and standard information theoretic measures, we derive two dimensions of incongruity -- ambiguity of meaning and distinctiveness of viewpoints -- and use them to predict humans' judgments of funniness. Results showed that both ambiguity and distinctiveness are significant predictors of humor. Additionally, our model automatically identifies specific features of a pun that make it amusing. We thus show how a probabilistic model of sentence comprehension can help explain essential features of the complex phenomenon of linguistic humor.},
	Author = {Kao, Justine T and Levy, Roger and Goodman, Noah D},
	Booktitle = {Proceedings of the Thirty-Fifth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {The Funny Thing About Incongruity: A Computational Model of Humor in Puns},
	Website = {https://web.stanford.edu/~ngoodman/papers/KaoLevyGoodman.pdf},
	Year = {2013}}

@inproceedings{lieder2013learned,
	Abstract = {In learned helplessness experiments, subjects first experience a lack of control in one situation, and then show learning deficits when performing or learning another task in another situation. Generalization, thus, is at the core of the learned helplessness phenomenon. Substantial experimental and theoretical effort has been invested into establishing that a state- and task-independent belief about controllability is necessary. However, to what extent generalization is also sufficient to explain the transfer has not been examined. Here, we show qualitatively and quantitatively that Bayesian learning of action-outcome contingencies at three levels of abstraction is sufficient to account for the key features of learned helplessness, including escape deficits and impairment of appetitive learning after inescapable shocks.},
	Author = {Lieder, Falk and Goodman, Noah D and Huys, Quentin JM},
	Booktitle = {Proceedings of the Thirty-Fifth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Learned helplessness and generalization},
	Website = {https://web.stanford.edu/~ngoodman/papers/LiederGoodmanHuys2013.pdf},
	Year = {2013}}

@misc{goodman2013principles,
	Abstract = {Probabilities describe degrees of belief, and probabilistic inference describes rational reasoning under uncertainty. It is no wonder, then, that probabilistic models have exploded onto the scene of modern artificial intelligence, cognitive science, and applied statistics: these are all sciences of inference under uncertainty. But as probabilistic models have become more sophisticated, the tools to formally describe them and to perform probabilistic inference have wrestled with new complexity. Just as programming beyond the simplest algorithms requires tools for abstraction and composition, complex probabilistic modeling requires new progress in model representation -- probabilistic programming languages. These languages provide compositional means for describing complex probability distributions; implementations of these languages provide generic inference engines: tools for performing efficient probabilistic inference over an arbitrary program.},
	Annote = {(Extended abstract of keynote talk.)},
	Author = {Goodman, Noah D},
	Booktitle = {POPL 2013},
	Title = {The principles and practice of probabilistic programming},
	Website = {https://web.stanford.edu/~ngoodman/papers/POPL2013-abstract.pdf},
	Year = {2013}}

@article{Goodman2013,
	Abstract = {Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to "invert" this model of the speaker. We apply this framework to model scalar implicature ("some" implies "not all," and "N" implies "not more than N"). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.},
	Author = {Goodman, N.D. and Stuhlm{\"u}ller, A.},
	Journal = {Topics in Cognitive Science},
	Page = {173--184},
	Title = {Knowledge and implicature: Modeling language understanding as social cognition},
	Volume = {5},
	Website = {https://web.stanford.edu/~ngoodman/papers/GS-TopiCS-2013.pdf},
	Year = {2013}}

@inproceedings{Lassiter2013,
	Abstract = {Relative adjectives in the positive form exhibit vagueness and context sensitivity. We suggest that these phenomena can be explained by the interaction of a free threshold variable in the meaning of the positive form with a probabilistic model of pragmatic inference. We describe a formal model of utterance interpretation as coordination, which jointly infers the value of the threshold variable and the intended meaning of the sentence. We report simulations exploring the effect of background statistical knowledge on adjective interpretation in this model. Motivated by these simulation results, we suggest that this approach can account for the correlation between scale structure and the relative/absolute distinction while also allowing for exceptions noted in previous work. Finally, we argue for a probabilistic explanation of why the sorites paradox is compelling with relative adjectives even though the second premise is false on a universal interpretation, and show that this account predicts Kennedy's (2007) observation that the sorites paradox is more compelling with relative than with absolute adjectives.},
	Author = {D. Lassiter and N. D. Goodman},
	Booktitle = {Semantics and {L}inguistic {T}heory {(SALT)} 23},
	Date-Added = {2013-08-17 20:32:09 +0000},
	Date-Modified = {2013-08-17 20:33:01 +0000},
	Title = {Context, scale structure, and statistics in the interpretation of positive-form adjectives},
	Website = {http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/view/2658/2404},
	Year = {2013}}

@article{Stuhlmueller2013,
	Abstract = {A wide range of human reasoning patterns can be explained as conditioning in probabilistic models; however, conditioning has traditionally been viewed as an operation applied to such models, not represented in such models. We describe how probabilistic programs can explicitly represent conditioning as part of a model. This enables us to describe reasoning about others' reasoning using nested conditioning. Much of human reasoning is about the beliefs, desires, and intentions of other people; we use probabilistic programs to formalize these inferences in a way that captures the flexibility and inherent uncertainty of reasoning about other agents. We express examples from game theory, artificial intelligence, and linguistics as recursive probabilistic programs and illustrate how this representation language makes it easy to explore new directions in each of these fields. We discuss the algorithmic challenges posed by these kinds of models and describe how Dynamic Programming techniques can help address these challenges.},
	Author = {A. Stuhlm{\"u}ller and N. D. Goodman},
	Date-Added = {2013-08-17 17:09:29 +0000},
	Date-Modified = {2013-08-17 17:13:18 +0000},
	Journal = {J. Cognitive Systems Research},
	Title = {Reasoning about Reasoning by Nested Conditioning: Modeling Theory of Mind with Probabilistic Programs},
	Website = {https://web.stanford.edu/~ngoodman/papers/StuhlmuellerGoodman-CogSys-2013.pdf},
	Year = {2013}}

@article{Hamlin2013,
	Abstract = {Evaluating individuals based on their proand anti-social behaviors is fundamental to successful human interaction. Recent research suggests that even preverbal infants engage in social evaluation; however, it remains an open question whether infants' judgments are driven uniquely by an analysis of the mental states that motivate others' helpful and unhelpful actions, or whether non-mentalistic inferences are at play. Here we present evidence from 10-month-olds, motivated and supported by a Bayesian computational model, for mentalistic social evaluation in the first year of life. A video abstract of this article can be viewed at <a href="//youtu.be/rD_Ry5oqCYE">//youtu.be/rD_Ry5oqCYE</a>},
	Author = {Hamlin, Kiley J and Ullman, Tomer and Tenenbaum, Josh B. and Goodman, Noah D. and Baker, Chris},
	Date-Added = {2013-08-17 20:15:14 +0000},
	Date-Modified = {2013-08-17 20:15:48 +0000},
	Journal = {{D}evelopmental {S}cience},
	Number = {2},
	Pages = {209--226},
	Publisher = {Wiley Online Library},
	Title = {The mentalistic basis of core social cognition: experiments in preverbal infants and a computational model},
	Volume = {16},
	Website = {https://web.stanford.edu/~ngoodman/papers/HamlinUllmanetalDevSci2013.pdf},
	Year = {2013}}

@article{seiver2013did,
	Abstract = {Children rely on both evidence and prior knowledge to make physical causal inferences; this study explores whether they make attributions about others' behavior in the same manner. A total of one hundred and fifty-nine 4- and 6-year-olds saw 2 dolls interacting with 2 activities, and explained the dolls' actions. In the person condition, each doll acted consistently across activities, but differently from each other. In the situation condition, the two dolls acted differently for each activity, but both performed the same actions. Both age groups provided more "person" explanations (citing features of the doll) in the person condition than in the situation condition. In addition, 6-year-olds showed an overall bias toward "person" explanations. As in physical causal inference, social causal inference combines covariational evidence and prior knowledge.},
	Author = {Seiver, Elizabeth and Gopnik, Alison and Goodman, Noah D},
	Journal = {Child {D}evelopment},
	Number = {2},
	Pages = {443--454},
	Title = {Did she jump because she was the big sister or because the trampoline was safe? Causal inference and the development of social attribution},
	Volume = {84},
	Website = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8624.2012.01865.x/full},
	Year = {2013}}

@inproceedings{lieder2012burn,
	Abstract = {Bayesian inference provides a unifying framework for learning, reasoning, and decision making. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate timeaccuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We characterize the optimal time-accuracy tradeoff mathematically in terms of the number of iterations and the resulting bias as functions of time cost, error cost, and the difficulty of the inference problem. We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as "burn-in". Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions match published data on anchoring in numerical estimation tasks. In conclusion, resource-rationality -- the optimal use of finite computational resources -- naturally leads to a biased mind.},
	Author = {Lieder, Falk and Griffiths, Thomas L and Goodman, Noah D},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {2699--2707},
	Title = {Burn-in, bias, and the rationality of anchoring},
	Website = {https://web.stanford.edu/~ngoodman/papers/LiederGriffithsGoodman2013NIPS.pdf},
	Year = {2012}}

@article{Ullman2012,
	Abstract = {We present an algorithmic model for the development of children's intuitive theories within a hierarchical Bayesian framework, where theories are described as sets of logical laws generated by a probabilistic context-free grammar. We contrast our approach with connectionist and other emergentist approaches to modeling cognitive development: while their subsymbolic representations provide a smooth error surface that supports efficient gradient-based learning, our symbolic representations are better suited to capturing children's intuitive theories but give rise to a harder learning problem, which can only be solved by exploratory search. Our algorithm attempts to discover the theory that best explains a set of observed data by performing stochastic search at two levels of abstraction: an outer loop in the space of theories, and an inner loop in the space of explanations or models generated by each theory given a particular dataset. We show that this stochastic search is capable of learning appropriate theories in several everyday domains, and discuss its dynamics in the context of empirical studies of children's learning.},
	Author = {T. Ullman and N. D. Goodman and J. B. Tenenbaum},
	Journal = {Cognitive {D}evelopment},
	Title = {Theory learning as stochastic search in the language of thought},
	Website = {http://www.mit.edu/~tomeru/papers/tlss-final.pdf},
	Year = {2012}}

@inproceedings{stuhlmuller2012dynamic,
	Abstract = {We describe a dynamic programming algorithm for computing the marginal distribution of discrete probabilistic programs. This algorithm takes a functional interpreter for an arbitrary probabilistic programming language and turns it into an efficient marginalizer. Because direct caching of sub-distributions is impossible in the presence of recursion, we build a graph of dependencies between sub-distributions. This factored sum-product network makes (potentially cyclic) dependencies between subproblems explicit, and corresponds to a system of equations for the marginal distribution. We solve these equations by fixed-point iteration in topological order. We illustrate this algorithm on examples used in teaching probabilistic models, computational cognitive science research, and game theory.},
	Author = {Stuhlm{\"u}ller, Andreas and Goodman, Noah D},
	Journal = {Second Statistical Relational AI workshop at UAI 2012 (StaRAI-12)},
	Title = {A dynamic programming algorithm for inference in recursive probabilistic programs},
	Website = {https://arxiv.org/abs/1206.3555},
	Year = {2012}}

@inproceedings{talton2012learning,
	Abstract = {Design patterns have proven useful in many creative fields, providing content creators with archetypal, reusable guidelines to leverage in projects. Creating such patterns, however, is a time-consuming, manual process, typically relegated to a few experts in any given domain. In this paper, we describe an algorithmic method for learning design patterns directly from data using techniques from natural language processing and structured concept learning. Given a set of labeled, hierarchical designs as input, we induce a probabilistic formal grammar over these exemplars. Once learned, this grammar encodes a set of generative rules for the class of designs, which can be sampled to synthesize novel artifacts. We demonstrate the method on geometric models and Web pages, and discuss how the learned patterns can drive new interaction mechanisms for content creators.},
	Annote = {<b>[Nominated for Best Paper Award.]</b>},
	Author = {Talton, Jerry and Yang, Lingfeng and Kumar, Ranjitha and Lim, Maxine and Goodman, Noah D and Mech, R},
	Booktitle = {Proceedings of the 25th annual ACM symposium on User interface software and technology},
	Pages = {63--74},
	Title = {Learning design patterns with bayesian grammar induction},
	Website = {https://web.stanford.edu/~ngoodman/papers/Talton12LDPBG.pdf},
	Year = {2012}}

@article{frank2012predicting,
	Abstract = {One of the most astonishing features of human language is its capacity to convey information efficiently in context. Many theories provide informal accounts of communicative inference, yet there have been few successes in making precise, quantitative predictions about pragmatic reasoning. We examine judgments about simple referential communication games, modeling behavior in these games by assuming that speakers attempt to be informative, and that listeners use Bayesian inference to recover speakers' intended referents. Our model provides a close, parameter-free fit to human judgments, suggesting that using information-theoretic tools to predict pragmatic reasoning may lead to more effective formal models of communication.},
	Author = {Frank, Michael C and Goodman, Noah D},
	Journal = {Science},
	Number = {6084},
	Pages = {998--998},
	Publisher = {American Association for the Advancement of Science},
	Title = {Predicting pragmatic reasoning in language games},
	Volume = {336},
	Website = {https://web.stanford.edu/~ngoodman/papers/FrankGoodman-Science2012.pdf},
	Year = {2012}}

@book{tenenbaumreverseengineering,
	Annote = {(in prep)},
	Author = {J. B. Tenenbaum and T. L. Griffiths and N. Chater and C. Kemp and N. D. Goodman and A. Yuille},
	Title = {Reverse engineering the mind: the Bayesian approach},
	Year = {in prep}}

@inproceedings{yeh2012synthesizing,
	Abstract = {We present a novel Markov chain Monte Carlo (MCMC) algorithm that generates samples from transdimensional distributions encoding complex constraints. We use factor graphs, a type of graphical model, to encode constraints as factors. Our proposed MCMC method, called locally annealed reversible jump MCMC, exploits knowledge of how dimension changes affect the structure of the factor graph. We employ a sequence of annealed distributions during the sampling process, allowing us to explore the state space across different dimensionalities more freely. This approach is motivated by the application of layout synthesis where relationships between objects are characterized as constraints. In particular, our method addresses the challenge of synthesizing open world layouts where the number of objects are not fixed and optimal configurations for different numbers of objects may be drastically different. We demonstrate the applicability of our approach on two open world layout synthesis problems: coffee shops and golf courses.},
	Author = {Yeh, Yi-Ting and Yang, Lingfeng and Watson, Matthew and Goodman, Noah D and Hanrahan, Pat},
	Journal = {SIGGRAPH 2012},
	Number = {4},
	Pages = {56},
	Title = {Synthesizing open worlds with constraints using locally annealed reversible jump MCMC},
	Volume = {31},
	Website = {https://web.stanford.edu/~ngoodman/papers/owl.pdf},
	Year = {2012}}

@article{shafto2012learning,
	Abstract = {From early childhood, human beings learn not only from collections of facts about the world, but also in social contexts: from observation of other people, from communication, and from explicit teaching. In these contexts, the data are the result of human actions -- actions that come about because of people's goals and intentions. To interpret the implications of others' actions correctly, learners must understand the people generating the data. Most models of learning, however, assume that data are randomly collected facts about the world, and cannot explain how social contexts influence learning. We provide a Bayesian analysis of learning from knowledgeable others, which formalizes how a learner may reason from a person's actions and goals to infer the actor's knowledge about the world. We illustrate this framework using two examples from causal learning and conclude by discussing the implications for cognition, social reasoning, and cognitive development.},
	Author = {Shafto, Patrick and Goodman, Noah D and Frank, Michael C},
	Journal = {Perspectives on Psychological Science},
	Number = {4},
	Pages = {341--351},
	Publisher = {Sage Publications},
	Title = {Learning from others: The consequences of psychological reasoning for human learning},
	Volume = {7},
	Website = {https://web.stanford.edu/~ngoodman/papers/shaftoperspectives.pdf},
	Year = {2012}}

@inproceedings{Goodman2012knowledge,
	Abstract = {Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to "invert" this model of the speaker. We apply this framework to model scalar implicature ("some" implies "not all," and "N" implies "not more than N"). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.},
	Annote = {<b>[Winner of the 2012 Cognitive Science Society computational modeling prize for Language.]</b>},
	Author = {Goodman, N.D. and Stuhlm{\"u}ller, A.},
	Date-Added = {2013-07-22 14:48:45 +0000},
	Date-Modified = {2013-07-22 14:48:56 +0000},
	Journal = {Proceedings of the Thirty-Fourth Annual Conference of the {C}ognitive {S}cience {S}ociety.},
	Title = {Knowledge and implicature: Modeling language understanding as social cognition},
	Website = {https://web.stanford.edu/~ngoodman/papers/KnowledgeImplicature-v2.pdf},
	Year = {2012}}

@inproceedings{bergen2012s,
	Abstract = {We investigate the effects of alternative utterances on pragmatic interpretation of language. We focus on two specific cases: specificity implicatures (less specific utterances imply the negation of more specific utterances) and Horn implicatures (more complex utterances are assigned to less likely meanings). We present models of these phenomena in terms of recursive social reasoning. Our most sophisticated model is not only able to handle specificity implicature but is also the first formal account of Horn implicatures that correctly predicts human behavior in signaling games with no prior conventions, without appeal to specialized equilibrium selection criteria. Two experiments provide evidence that these implicatures are generated in the absence of prior linguistic conventions or language evolution. Taken together, our modeling and experimental results suggest that the pragmatic effects of alternative utterances can be driven by cooperative social reasoning.},
	Author = {Bergen, Leon and Goodman, Noah D and Levy, Roger},
	Journal = {Proceedings of the thirty-fourth annual conference of the {C}ognitive {S}cience {S}ociety},
	Title = {That's what she (could have) said: How alternative utterances affect language use},
	Website = {https://web.stanford.edu/~ngoodman/papers/BergenEtAl2012.pdf},
	Year = {2012}}

@inproceedings{gerstenberg2012ping,
	Abstract = {How do people make inferences from complex patterns of evidence across diverse situations? What does a computational model need in order to capture the abstract knowledge people use for everyday reasoning? In this paper, we explore a novel modeling framework based on the probabilistic language of thought (PLoT) hypothesis, which conceptualizes thinking in terms of probabilistic inference over compositionally structured representations. The core assumptions of the PLoT hypothesis are realized in the probabilistic programming language Church (Goodman, Mansinghka, Roy, Bonawitz, & Tenenbaum, 2008). Using "ping pong tournaments" as a case study, we show how a single Church program concisely represents the concepts required to specify inferences from diverse patterns of evidence. In two experiments, we demonstrate a very close fit between our model's predictions and participants' judgments. Our model accurately predicts how people reason with confounded and indirect evidence and how different sources of information are integrated.},
	Author = {Gerstenberg, Tobias and Goodman, Noah D},
	Journal = {Proceedings of the 34th annual conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Ping pong in Church: Productive use of concepts in human probabilistic inference},
	Website = {https://web.stanford.edu/~ngoodman/papers/GerstenbergGoodman2012.pdf},
	Year = {2012}}

@inproceedings{gerstenberg2012noisy,
	Abstract = {There is a long tradition in both philosophy and psychology to separate process accounts from dependency accounts of causation. In this paper, we motivate a unifying account that explains people's causal attributions in terms of counterfactuals defined over probabilistic generative models. In our experiments, participants see two billiard balls colliding and indicate to what extent ball A caused/prevented ball B to go through a gate. Our model predicts that people arrive at their causal judgments by comparing what actually happened with what they think would have happened, had the collision between A and B not taken place. Participants' judgments about what would have happened are highly correlated with a noisy model of Newtonian physics. Using those counterfactual judgments, we can predict participants' cause and prevention judgments very accurately (r = .99). Our framework also allows us to capture intrinsically counterfactual judgments such as almost caused/prevented.},
	Author = {Gerstenberg, Tobias and Goodman, Noah and Lagnado, David A and Tenenbaum, Joshua B},
	Booktitle = {Proceedings of the Thirty-Fourth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Noisy Newtons: Unifying process and dependency accounts of causal attribution},
	Website = {https://web.stanford.edu/~ngoodman/papers/physics_cogsci2nd.pdf},
	Year = {2012}}

@inproceedings{lassiter2012many,
	Abstract = {Previous research (Heit & Rotello, 2010; Rips, 2001; Rotello & Heit, 2009) has suggested that differences between inductive and deductive reasoning cannot be explained by probabilistic theories, and instead support two-process accounts of reasoning. We provide a probabilistic model that predicts the observed non-linearities and makes quantitative predictions about responses as a function of argument strength. Predictions were tested using a novel experimental paradigm that elicits the previously-reported response patterns with a minimal manipulation, changing only one word between conditions. We also found a good fit with quantitative model predictions, indicating that a probabilistic theory of reasoning can account in a clear and parsimonious way for qualitative and quantitative data previously argued to falsify them. We also relate our model to recent work in linguistics, arguing that careful attention to the semantics of language used to pose reasoning problems will sharpen the questions asked in the psychology of reasoning.},
	Author = {Lassiter, Daniel and Goodman, Noah D},
	Journal = {34th Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {How many kinds of reasoning? Inference, probability, and natural language semantics},
	Website = {https://web.stanford.edu/~ngoodman/papers/LassiterGoodman12.pdf},
	Year = {2012}}

@article{piantadosi2012bootstrapping,
	Abstract = {In acquiring number words, children exhibit a qualitative leap in which they transition from understanding a few number words, to possessing a rich system of interrelated numerical concepts. We present a computational framework for understanding this inductive leap as the consequence of statistical inference over a sufficiently powerful representational system. We provide an implemented model that is powerful enough to learn number word meanings and other related conceptual systems from naturalistic data. The model shows that bootstrapping can be made computationally and philosophically wellfounded as a theory of number learning. Our approach demonstrates how learners may combine core cognitive operations to build sophisticated representations during the course of development, and how this process explains observed developmental patterns in number word learning.},
	Author = {Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Cognition},
	Number = {2},
	Pages = {199--217},
	Title = {Bootstrapping in a language of thought: A formal model of numerical concept learning},
	Volume = {123},
	Website = {https://web.stanford.edu/~ngoodman/papers/COGNIT2374.pdf},
	Year = {2012}}

@article{scontras2012comparing,
	Abstract = {What does it mean to compare sets of objects along a scale, for example by saying "the men are taller than the women"? We explore comparison of pluralities in two experiments, eliciting comparison judgments while varying the properties of the members of each set. We find that a plurality is judged as "bigger" when the mean size of its members is larger than the mean size of the competing plurality. These results are incompatible with previous accounts, in which plural comparison is inferred from many instances of singular comparison between the members of the sets (Matushansky and Ruys, 2006). Our results suggest the need for a type of predication that ascribes properties to plural entities, not just individuals, based on aggregate statistics of their members. More generally, these results support the idea that sets and their properties are actively represented as single units.},
	Author = {Scontras, Gregory and Graff, Peter and Goodman, Noah D},
	Journal = {Cognition},
	Number = {1},
	Pages = {190--197},
	Title = {Comparing pluralities},
	Volume = {123},
	Website = {https://web.stanford.edu/~ngoodman/papers/ScontrasGraffGoodman-ComparingPluralities.pdf},
	Year = {2012}}

@techreport{hwang2011inducing,
	Abstract = {This report outlines an approach to learning generative models from data. We express models as probabilistic programs, which allows us to capture abstract patterns within the examples. By choosing our language for programs to be an extension of the algebraic data type of the examples, we can begin with a program that generates all and only the examples. We then introduce greater abstraction, and hence generalization, incrementally to the extent that it improves the posterior probability of the examples given the program. Motivated by previous approaches to model merging and program induction, we search for such explanatory abstractions using program transformations. We consider two types of transformation: Abstraction merges common subexpressions within a program into new functions (a form of anti-unification). Deargumentation simplifies functions by reducing the number of arguments. We demonstrate that this approach finds key patterns in the domain of nested lists, including parameterized sub-functions and stochastic recursion.},
	Author = {Hwang, Irvin and Stuhlm{\"u}ller, Andreas and Goodman, Noah D},
	Journal = {Technical report: arXiv:1110.5667},
	Title = {Inducing probabilistic programs by Bayesian program merging},
	Website = {http://arxiv.org/abs/1110.5667},
	Year = {2011}}

@article{cook2011science,
	Abstract = {Probabilistic models of expected information gain require integrating prior knowledge about causal hypotheses with knowledge about possible actions that might generate data relevant to those hypotheses. Here we looked at whether preschoolers (mean: 54 months) recognize "action possibilities" (affordances) in the environment that allow them to isolate variables when there is information to be gained. By manipulating the physical properties of the stimuli, we were able to affect the degree to which candidate variables could be isolated; by manipulating the base rate of candidate causes, we were able to affect the potential for information gain. Children's exploratory play was sensitive to both manipulations: given unambiguous evidence children played indiscriminately and rarely tried to isolate candidate causes; given ambiguous evidence, children both selected (Experiment 1) and designed (Experiment 2) informative interventions.},
	Author = {Cook, Claire and Goodman, Noah D and Schulz, Laura E},
	Journal = {Cognition},
	Number = {3},
	Pages = {341--349},
	Title = {Where science starts: Spontaneous experiments in preschoolers' exploratory play},
	Volume = {120},
	Website = {https://web.stanford.edu/~ngoodman/papers/CookGoodmanSchulz2011.pdf},
	Year = {2011}}

@article{BonawitzEtAl2011-Cognition,
	Abstract = {Motivated by computational analyses, we look at how teaching affects exploration and discovery. In Experiment 1, we investigated children's exploratory play after an adult pedagogically demonstrated a function of a toy, after an interrupted pedagogical demonstration, after a na{\"\i}ve adult demonstrated the function, and at baseline. Preschoolers in the pedagogical condition focused almost exclusively on the target function; by contrast, children in the other conditions explored broadly. In Experiment 2, we show that children restrict their exploration both after direct instruction to themselves and after overhearing direct instruction given to another child; they do not show this constraint after observing direct instruction given to an adult or after observing a non-pedagogical intentional action. We discuss these findings as the result of rational inductive biases. In pedagogical contexts, a teacher's failure to provide evidence for additional functions provides evidence for their absence; such contexts generalize from child to child (because children are likely to have comparable states of knowledge) but not from adult to child. Thus, pedagogy promotes efficient learning but at a cost: children are less likely to perform potentially irrelevant actions but also less likely to discover novel information.},
	Author = {Bonawitz, Elizabeth and Shafto, Patrick and Gweon, Hyowon and Goodman, Noah D and Spelke, Elizabeth and Schulz, Laura},
	Journal = {Cognition},
	Number = {3},
	Pages = {322--330},
	Title = {The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery},
	Volume = {120},
	Website = {papers/BonawitzEtAl2011-Cognition.pdf},
	Year = {2011}}

@article{chater2011imaginary,
	Abstract = {The prominence of Bayesian modeling of cognition has increased recently largely because of mathematical advances in specifying and deriving predictions from complex probabilistic models. Much of this research aims to demonstrate that cognitive behavior can be explained from rational principles alone, without recourse to psychological or neurological processes and representations. We note commonalities between this rational approach and other movements in psychology -- namely, Behaviorism and evolutionary psychology -- that set aside mechanistic explanations or make use of optimality assumptions. Through these comparisons, we identify a number of challenges that limit the rational program's potential contribution to psychological theory. Specifically, rational Bayesian models are significantly unconstrained, both because they are uninformed by a wide range of process-level data and because their assumptions about the environment are generally not grounded in empirical measurement. The psychological implications of most Bayesian models are also unclear. Bayesian inference itself is conceptually trivial, but strong assumptions are often embedded in the hypothesis sets and the approximation algorithms used to derive model predictions, without a clear delineation between psychological commitments and implementational details. Comparing multiple Bayesian models of the same task is rare, as is the realization that many Bayesian models recapitulate existing (mechanistic level) theories. Despite the expressive power of current Bayesian models, we argue they must be developed in conjunction with mechanistic considerations to offer substantive explanations of cognition. We lay out several means for such an integration, which take into account the representations on which Bayesian inference operates, as well as the algorithms and heuristics that carry it out. We argue this unification will better facilitate lasting contributions to psychological theory, avoiding the pitfalls that have plagued previous theoretical movements.},
	Annote = {(Commentary on Jones and Love.)},
	Author = {Chater, Nick and Goodman, Noah and Griffiths, Thomas L and Kemp, Charles and Oaksford, Mike and Tenenbaum, Joshua B},
	Journal = {Behavioral and Brain Sciences},
	Number = {04},
	Pages = {194--196},
	Publisher = {Cambridge University Press},
	Title = {The imaginary fundamentalists: The unshocking truth about Bayesian cognitive science},
	Volume = {34},
	Website = {https://journals.cambridge.org/download.php?file=%2FBBS%2FBBS34_04%2FS0140525X11000239a.pdf&code=1a1eedeede20981ce332fec41a70bb64#page=-142},
	Year = {2011}}

@inproceedings{wingate2011bayesian,
	Abstract = {We consider the problem of learning to act in partially observable, continuous-state-and-action worlds where we have abstract prior knowledge about the structure of the optimal policy in the form of a distribution over policies. Using ideas from planning-as-inference reductions and Bayesian unsupervised learning, we cast Markov Chain Monte Carlo as a stochastic, hill-climbing policy search algorithm. Importantly, this algorithm's search bias is directly tied to the prior and its MCMC proposal kernels, which means we can draw on the full Bayesian toolbox to express the search bias, including nonparametric priors and structured, recursive processes like grammars over action sequences. Furthermore, we can reason about uncertainty in the search bias itself by constructing a hierarchical prior and reasoning about latent variables that determine the abstract structure of the policy. This yields an adaptive search algorithm -- our algorithm learns to learn a structured policy efficiently. We show how inference over the latent variables in these policy priors enables intra- and intertask transfer of abstract knowledge. We demonstrate the flexibility of this approach by learning meta search biases, by constructing a nonparametric finite state controller to model memory, by discovering motor primitives using a simple grammar over primitive actions, and by combining all three.},
	Annote = {<b>[Winner of the Best Poster prize]</b>},
	Author = {Wingate, David and Goodman, Noah D and Roy, Daniel M and Kaelbling, Leslie P and Tenenbaum, Joshua B},
	Booktitle = {Proceedings of the Twenty-Second international joint conference on Artificial Intelligence (IJCAI 11)},
	Title = {Bayesian policy search with policy priors},
	Website = {https://web.stanford.edu/~ngoodman/papers/WingateEtAl-PolicyPrios.pdf},
	Year = {2011}}

@article{tenenbaum2011grow,
	Abstract = {In coming to understand the world -- in learning concepts, acquiring language, and grasping causal relations -- our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?},
	Author = {Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L and Goodman, Noah D},
	Journal = {Science},
	Number = {6022},
	Pages = {1279--1285},
	Title = {How to grow a mind: Statistics, structure, and abstraction},
	Volume = {331},
	Website = {https://web.stanford.edu/~ngoodman/papers/tkgg-science11-reprint.pdf},
	Year = {2011}}

@inproceedings{wingate2011nonstandard,
	Abstract = {Probabilistic programming languages allow modelers to specify a stochastic process using syntax that resembles modern programming languages. Because the program is in machine-readable format, a variety of techniques from compiler design and program analysis can be used to examine the structure of the distribution represented by the probabilistic program. We show how nonstandard interpretations of probabilistic programs can be used to craft efficient inference algorithms: information about the structure of a distribution (such as gradients or dependencies) is generated as a monad-like side computation while executing the program. These interpretations can be easily coded using special-purpose objects and operator overloading. We implement two examples of nonstandard interpretations in two different languages, and use them as building blocks to construct inference algorithms: automatic differentiation, which enables gradient based methods, and provenance tracking, which enables efficient construction of global proposals.},
	Author = {Wingate, David and Goodman, Noah D and Stuhlmueller, Andreas and Siskind, Jeffrey Mark},
	Booktitle = {Advances in Neural Information Processing Systems 23},
	Pages = {1152--1160},
	Title = {Nonstandard Interpretations of Probabilistic Programs for Efficient Inference},
	Website = {https://web.stanford.edu/~ngoodman/papers/WGSS-NIPS11.pdf},
	Year = {2011}}

@inproceedings{stiller2011ad,
	Abstract = {Linguistic communication relies on pragmatic implicatures such as the inference that if "some students passed the test," not all did. Yet young children perform poorly on tests of implicature, especially scalar implicatures using "some" and "all," until quite late in development. We investigate the origins of scalar implicature using tasks in which the scale arises from real-world context rather than conventional contrasts between lexical items. Experiment 1 shows that these ad-hoc implicatures are easy for preschool children, suggesting that children have an early competence at pragmatic inference, and that failures in standard scalar implicature tasks are due instead to problems contrasting lexical items. Experiments 2 and 3 compare a Gricean, counterfactual account of implicature with a linguistic alternatives account and find that neither predicts effects of contextual informativeness. We conclude that an account of pragmatic implicature must integrate world knowledge, linguistic structure, and social reasoning.},
	Author = {Stiller, Alex and Goodman, Noah D and Frank, Michael C},
	Journal = {Proceedings of the 33rd Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Ad-hoc scalar implicature in adults and children},
	Website = {https://web.stanford.edu/~ngoodman/papers/SGF-cogsci2011.pdf},
	Year = {2011}}

@inproceedings{o2011productivity,
	Abstract = {We present a Bayesian model of the mirror image problems of linguistic productivity and reuse. The model, known as Fragment Grammar, is evaluated against several morphological datasets; its performance is compared to competing theoretical accounts including full-parsing, full-listing, and exemplar-based models. The model is able to learn the correct patterns of productivity and reuse for two very different systems: the English past tense which is characterized by a sharp dichotomy in productivity between regular and irregular forms and English derivational morphology which is characterized by a graded cline from very productive (-ness) to very unproductive (-th).},
	Annote = {<b>[Winner of the 2011 Cognitive Science Society computational modeling prize for Language.]</b>},
	Author = {O'donnell, Timothy J and Snedeker, Jesse and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Proceedings of the Thirty-Third Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Productivity and reuse in language},
	Website = {https://web.stanford.edu/~ngoodman/papers/odonnell-cogsci11.pdf},
	Year = {2011}}

@inproceedings{Wingate2011a,
	Abstract = {We describe a general method of transforming arbitrary programming languages into probabilistic programming languages with straightforward MCMC inference engines. Random choices in the program are "named" with information about their position in an execution trace; these names are used in conjunction with a database holding values of random variables to implement MCMC inference in the space of execution traces. We encode naming information using lightweight source-to-source compilers. Our method enables us to reuse existing infrastructure (compilers, profilers, etc.) with minimal additional code, implying fast models with low development overhead. We illustrate the technique on two languages, one functional and one imperative: Bher, a compiled version of the Church language which eliminates interpretive overhead of the original MIT-Church implementation, and Stochastic Matlab, a new open-source language.},
	Author = {Wingate, D. and Stuhlm{\"u}ller, A. and Goodman, N.D.},
	Booktitle = {Proceedings of the 14th international conference on Artificial Intelligence and Statistics},
	Pages = {770--778},
	Title = {Lightweight implementations of probabilistic programming languages via transformational compilation},
	Website = {http://jmlr.csail.mit.edu/proceedings/papers/v15/wingate11a/wingate11a.pdf},
	Year = {2011}}

@article{goodman2011learning,
	Abstract = {The very early appearance of abstract knowledge is often taken as evidence for innateness. We explore the relative learning speeds of abstract and specific knowledge within a Bayesian framework, and the role for innate structure. We focus on knowledge about causality, seen as a domain-general intuitive theory, and ask whether this knowledge can be learned from cooccurrence of events. We begin by phrasing the causal Bayes nets theory of causality, and a range of alternatives, in a logical language for relational theories. This allows us to explore simultaneous inductive learning of an abstract theory of causality and a causal model for each of several causal systems. We find that the correct theory of causality can be learned relatively quickly, often becoming available before specific causal theories have been learned -- an effect we term the blessing of abstraction. We then explore the effect of providing a variety of auxiliary evidence, and find that a collection of simple "perceptual input analyzers" can help to bootstrap abstract knowledge. Together these results suggest that the most efficient route to causal knowledge may be to build in not an abstract notion of causality, but a powerful inductive learning mechanism and a variety of perceptual supports. While these results are purely computational, they have implications for cognitive development, which we explore in the conclusion.},
	Author = {Goodman, Noah D and Ullman, Tomer D and Tenenbaum, Joshua B},
	Journal = {Psychological {R}eview},
	Number = {1},
	Pages = {110},
	Publisher = {American Psychological Association},
	Title = {Learning a theory of causality.},
	Volume = {118},
	Website = {https://web.stanford.edu/~ngoodman/papers/LTBC_psychreview_final.pdf},
	Year = {2011}}

@article{DesrochersEtAl2010-PNAS,
	Abstract = {Habits and rituals are expressed universally across animal species. These behaviors are advantageous in allowing sequential behaviors to be performed without cognitive overload, and appear to rely on neural circuits that are relatively benign but vulnerable to takeover by extreme contexts, neuropsychiatric sequelae, and processes leading to addiction. Reinforcement learning (RL) is thought to underlie the formation of optimal habits. However, this theoretic formulation has principally been tested experimentally in simple stimulus-response tasks with relatively few available responses. We asked whether RL could also account for the emergence of habitual action sequences in realistically complex situations in which no repetitive stimulus-response links were present and in which many response options were present. We exposed na{\"\i}ve macaque monkeys to such experimental conditions by introducing a unique free saccade scan task. Despite the highly uncertain conditions and no instruction, the monkeys developed a succession of stereotypical, self-chosen saccade sequence patterns. Remarkably, these continued to morph for months, long after session-averaged reward and cost (eye movement distance) reached asymptote. Prima facie, these continued behavioral changes appeared to challenge RL. However, trial-by-trial analysis showed that pattern changes on adjacent trials were predicted by lowered cost, and RL simulations that reduced the cost reproduced the monkeys' behavior. Ultimately, the patterns settled into stereotypical saccade sequences that minimized the cost of obtaining the reward on average. These findings suggest that brain mechanisms underlying the emergence of habits, and perhaps unwanted repetitive behaviors in clinical disorders, could follow RL algorithms capturing extremely local explore/exploit tradeoffs.},
	Author = {Desrochers, Theresa M and Jin, Dezhe Z and Goodman, Noah D and Graybiel, Ann M},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {47},
	Pages = {20512--20517},
	Publisher = {National Acad Sciences},
	Title = {Optimal habits can develop spontaneously through sensitivity to local cost},
	Volume = {107},
	Website = {papers/DesrochersEtAl2010-PNAS.pdf},
	Year = {2010}}

@article{KempEtAl2010-CognitiveScience,
	Author = {Kemp, Charles and Goodman, Noah D and Tenenbaum, Joshua B},
	Journal = {Cognitive Science},
	Number = {7},
	Pages = {1185--1243},
	Publisher = {Blackwell Publishing Ltd},
	Title = {Learning to learn causal models},
	Volume = {34},
	Website = {papers/KempEtAl2010-CognitiveScience.pdf},
	Year = {2010}}

@article{frank2010predicting,
	Author = {Frank, Michael and Kenney, Avril and Goodman, Noah and Tenenbaum, Joshua and Torralba, Antonio and Oliva, Aude},
	Journal = {Journal of Vision},
	Number = {7},
	Pages = {1241--1241},
	Publisher = {Association for Research in Vision and Ophthalmology},
	Title = {Predicting object and scene descriptions with an information-theoretic model of pragmatics},
	Volume = {10},
	Website = {http://jov.arvojournals.org/article.aspx?articleid=2138025},
	Year = {2010}}

@article{HendersonEtAl2010-PhilosSci,
	Abstract = {Hierarchical Bayesian models (HBMs) provide an account of Bayesian inference in a hierarchically structured hypothesis space. Scientific theories are plausibly regarded as organized into hierarchies in many cases, with higher levels sometimes called 'paradigms' and lower levels encoding more specific or concrete hypotheses. Therefore, HBMs provide a useful model for scientific theory change, showing how higher-level theory change may be driven by the impact of evidence on lower levels. HBMs capture features described in the Kuhnian tradition, particularly the idea that higher-level theories guide learning at lower levels. In addition, they help resolve certain issues for Bayesians, such as scientific preference for simplicity and the problem of new theories.},
	Author = {Henderson, Leah and Goodman, Noah D and Tenenbaum, Joshua B and Woodward, James F},
	Journal = {Philosophy of Science},
	Number = {2},
	Pages = {172--200},
	Title = {The Structure and Dynamics of Scientific Theories: A Hierarchical Bayesian Perspective},
	Volume = {77},
	Website = {papers/HendersonEtAl2010-PhilosSci.pdf},
	Year = {2010}}

@inproceedings{ShaftoEtAl2010-Cogsci,
	Abstract = {Much of human learning occurs in social situations, and among these, pedagogical situations may afford the most powerful learning. In pedagogical situations, a teacher chooses the concept that they are going to teach and the examples that they use to teach the concept. If learners know that a teacher is helpful and understands the implications, this could support strong inferences. In previous work, Shafto and Goodman (2008) proposed and tested a model of pedagogical data selection. We integrate special-purpose pedagogical expectations in this framework, and derive a task that allows independent assessment of pedagogical expectations. Two experiments contrast people's expectations about pedagogical and communicative situations. The results show that people's expectations differ in these situations, and that in pedagogical situations people expect teachers to present generalizable and semantically coherent knowledge. We discuss the implications for modeling learning in pedagogical settings, as well as for understanding human learning more broadly.},
	Author = {Shafto, Patrick and Goodman, Noah D and Gerstle, Ben and Ladusaw, Francy},
	Journal = {Proceedings of the Thirty-Second Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Prior expectations in pedagogical situations},
	Website = {papers/ShaftoEtAl2010-Cogsci.pdf},
	Year = {2010}}

@inproceedings{StuhlmullerEtAl2010-Cogsci,
	Abstract = {Many real world concepts, such as "car", "house", and "tree", are more than simply a collection of features. These objects are richly structured, defined in terms of systems of relations, subparts, and recursive embeddings. We describe an approach to concept representation and learning that attempts to capture such structured objects. This approach builds on recent probabilistic approaches, viewing concepts as generative processes, and on recent rule-based approaches, constructing concepts inductively from a language of thought. Concepts are modeled as probabilistic programs that describe generative processes; these programs are described in a compositional language. In an exploratory concept learning experiment, we investigate human learning from sets of tree-like objects generated by processes that vary in their abstract structure, from simple prototypes to complex recursions. We compare human categorization judgements to predictions of the true generative process as well as a variety of exemplar-based heuristics.},
	Author = {Stuhlm{\"u}ller, Andreas and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Proceedings of the Thirty-Second Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Learning structured generative concepts},
	Website = {papers/StuhlmullerEtAl2010-Cogsci.pdf},
	Year = {2010}}

@inproceedings{PiantadosiEtAl2010-Cogsci,
	Abstract = {We study concept learning for semantically-motivated, settheoretic concepts. We first present an experiment in which we show that subjects learn concepts which cannot be represented by a simple Boolean logic. We then present a computational model which is similarly capable of learning these concepts, and show that it provides a good fit to human learning curves. Additionally, we compare the performance of several potential representation languages which are richer than Boolean logic in predicting human response distributions.},
	Author = {Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Proceedings of the 32nd Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Pages = {859--864},
	Title = {Beyond Boolean logic: exploring representation languages for learning complex concepts},
	Website = {papers/PiantadosiEtAl2010-Cogsci.pdf},
	Year = {2010}}

@inproceedings{UllmanEtAl2010-Cogsci,
	Abstract = {We present an algorithmic model for the development of children's intuitive theories within a hierarchical Bayesian framework, where theories are described as sets of logical laws generated by a probabilistic context-free grammar. Our algorithm performs stochastic search at two levels of abstraction -- an outer loop in the space of theories, and an inner loop in the space of explanations or models generated by each theory given a particular dataset -- in order to discover the theory that best explains the observed data. We show that this model is capable of learning correct theories in several everyday domains, and discuss the dynamics of learning in the context of children's cognitive development.},
	Author = {Ullman, T.D. and Goodman, N.D. and Tenenbaum, J.B.},
	Booktitle = {Proceedings of Thirty Second Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Theory Acquisition as Stochastic Search},
	Website = {papers/UllmanEtAl2010-Cogsci.pdf},
	Year = {2010}}

@inproceedings{WingateEtAl2009-UncertaintyInArtificialIntelligence,
	Abstract = {We present the Infinite Latent Events Model, a nonparametric hierarchical Bayesian distribution over infinite dimensional Dynamic Bayesian Networks with binary state representations and noisy-OR-like transitions. The distribution can be used to learn structure in discrete timeseries data by simultaneously inferring a set of latent events, which events fired at each timestep, and how those events are causally linked. We illustrate the model on a sound factorization task, a network topology identification task, and a video game task.},
	Author = {Wingate, David and Goodman, Noah D and Roy, Daniel M and Tenenbaum, Joshua B},
	Booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
	Organization = {AUAI Press},
	Pages = {607--614},
	Title = {The infinite latent events model},
	Website = {papers/WingateEtAl2009-UncertaintyInArtificialIntelligence.pdf},
	Year = {2009}}

@inproceedings{GoodmanUllmanTenenbaum2009-Cogsci,
	Abstract = {We consider causality as a domain-general intuitive theory and ask whether this intuitive theory can be learned from cooccurrence of events. We begin by phrasing the causal Bayes nets theory of causality, and a range of alternatives, in a logical language for relational theories. This allows us to explore simultaneous inductive learning of an abstract theory of causality and a causal model for each of several causal systems. We find that the correct theory of causality can be learned relatively quickly, often becoming available before specific causal theories have been learned---an effect we term the "blessing of abstraction". We then explore the effect of providing a variety of auxiliary evidence, and find that a collection of simple "input analyzers" can help to bootstrap abstract knowledge. Together these results suggest that the most efficient route to causal knowledge may be to build in not an abstract notion of causality, but a powerful inductive learning mechanism and a variety of perceptual supports. While these results are purely computational, they have implications for cognitive development, which we explore in the conclusion.},
	Author = {Goodman, N. D. and Ullman, T. D. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of the 31st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Learning a theory of causality},
	Website = {papers/GoodmanUllmanTenenbaum2009-Cogsci.pdf},
	Year = {2009}}

@inproceedings{Vul2009,
	Abstract = {In many learning or inference tasks human behavior approximates that of a Bayesian ideal observer, suggesting that, at some level, cognition can be described as Bayesian inference. However, a number of findings have highlighted an intriguing mismatch between human behavior and standard assumptions about optimality: People often appear to make decisions based on just one or a few samples from the appropriate posterior probability distribution, rather than using the full distribution. Although sampling-based approximations are a common way to implement Bayesian inference, the very limited numbers of samples often used by humans seem insufficient to approximate the required probability distributions very accurately. Here, we consider this discrepancy in the broader framework of statistical decision theory, and ask: If people are making decisions based on samples -- but as samples are costly -- how many samples should people use to optimize their total expected or worst-case reward over a large number of decisions? We find that under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods. These results help to reconcile a large body of work showing sampling-based or probability matching behavior with the hypothesis that human cognition can be understood in Bayesian terms, and they suggest promising future directions for studies of resource-constrained cognition.},
	Author = {E. Vul and N. D. Goodman and T. L. Griffiths and J. B. Tenenbaum},
	Booktitle = {Proceedings of the 31st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Date-Added = {2009-11-02 16:37:59 -0500},
	Date-Modified = {2009-11-02 16:38:55 -0500},
	Title = {One and done: Globally optimal behavior from locally suboptimal decisions.},
	Website = {//stanford.edu/~ngoodman/papers/VulEtAl2009.pdf},
	Year = {2009}}

@article{Frank2009,
	Abstract = {Word learning is a "chicken and egg" problem. If a child could understand speakers' utterances, it would be easy to learn the meanings of individual words, and once a child knows what many words mean, it is easy to infer speakers' intended meanings. To the beginning learner, however, both individual word meanings and speakers' intentions are unknown. We describe a computational model of word learning that solves these two inference problems in parallel, rather than relying exclusively on either the inferred meanings of utterances or cross-situational word-meaning associations. We tested our model using annotated corpus data and found that it inferred pairings between words and object concepts with higher precision than comparison models. Moreover, as the result of making probabilistic inferences about speakers' intentions, our model explains a variety of behavioral phenomena described in the word-learning literature. These phenomena include mutual exclusivity, one-trial learning, cross-situational learning, the role of words in object individuation, and the use of inferred intentions to disambiguate reference.},
	Author = {Frank, M. C. and Goodman, N. D. and Tenenbaum, J. B.},
	Journal = {Psychological Science},
	Title = {Using speakers' referential intentions to model early cross-situational word learning},
	Website = {http://langcog.stanford.edu/papers/FGT-psychscience2009.pdf},
	Year = {2009}}

@techreport{ODonnellEtAl2009-CSAIL,
	Abstract = {Language relies on a division of labor between stored units and structure building operations which combine the stored units into larger structures. This division of labor leads to a tradeoff: more structure-building means less need to store while more storage means less need to compute structure. We develop a hierarchical Bayesian model called fragment grammar to explore the optimum balance between structure-building and reuse. The model is developed in the context of stochastic functional programming (SFP), and in particular, using a probabilistic variant of Lisp known as the Church programming language [17]. We show how to formalize several probabilistic models of language structure using Church, and how fragment grammar generalizes one of them--- adaptor grammars [21]. We conclude with experimental data with adults and preliminary evaluations of the model on natural language corpus data.},
	Author = {O'Donnell, Timothy J and Tenenbaum, Joshua B and Goodman, Noah D},
	Journal = {Technical Report MIT-CSAIL-TR-2009-013},
	Publisher = {Massachusetts Institute of Technology},
	Title = {Fragment grammars: Exploring computation and reuse in language},
	Website = {papers/ODonnellEtAl2009-CSAIL.pdf},
	Year = {2009}}

@inproceedings{SchmidtEtAl2009-Cogsci,
	Abstract = {What is tall? Like many words, tall is fundamentally compositional in meaning -- whether an item is tall depends on the statistical properties of the set of items it is being compared to. Despite preliminary evidence to this effect, no mathematical models of how tall operates in a given context have ever been empirically evaluated. We compare a number of statistical models to adults and children in judging which items are tall in various contexts, including both threshold-based and categorization-based models. We find that non-parametric statistical models of gradable adjectives best describes the judgments of people across a wide variety of contexts},
	Author = {Schmidt, Lauren A and Goodman, Noah D and Barner, David and Tenenbaum, Joshua B},
	Journal = {Proceedings of the 31st annual conference of the {C}ognitive {S}cience {S}ociety},
	Pages = {2759--2764},
	Title = {How tall is Tall? compositionality, statistics, and gradable adjectives},
	Website = {papers/SchmidtEtAl2009-Cogsci.pdf},
	Year = {2009}}

@inproceedings{FrankGoodmanTenenbaum2009-Cogsci,
	Abstract = {Utterances that are close in time are more likely to share the same referent. A word learner who is using information about the speaker's intended referents should be able to take advantage of this continuity and learn words more efficiently by aggregating information across multiple utterances. In the current study we use corpus data to explore the continuity of reference in caregivers' speech to infants. We measure the degree of referential continuity in two corpora and then use regression modeling to test whether reference continuity is informative about speakers' referential intentions. We conclude by developing a simple discourse-continuity prior within a Bayesian model of word learning. Our results suggest that discourse continuity may be a valuable information source in early word learning.},
	Author = {Frank, Michael C and Goodman, Noah D and Tenenbaum, Joshua B and Fernald, Anne},
	Journal = {Proceedings of the 31st Annual {C}ognitive {S}cience {S}ociety},
	Title = {Continuity of discourse provides information for word learning},
	Website = {papers/FrankGoodmanTenenbaum2009-Cogsci.pdf},
	Year = {2009}}

@inproceedings{GoodmanEtAl2009-Cogsci,
	Abstract = {The acquisition of causal knowledge is a primary goal of childhood; yet most of this knowledge is known already to adults. We argue that causal learning which leverages social reasoning is a rapid and important route to knowledge. We present a computational model integrating knowledge about causality with knowledge about intentional agency, but using a domaingeneral mechanism for reasoning. Inference in this model predicts qualitatively different learning than an equivalent model based on causality alone or a hybrid causal-encoding model. We test these predictions experimentally with adult participants, and discuss the relation of these results to the developmental phenomenon of over-imitation.},
	Author = {Goodman, N. D. and Baker, C. L. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of the 31st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Cause and intent: Social reasoning in causal learning},
	Website = {papers/GoodmanEtAl2009-Cogsci.pdf},
	Year = {2009}}

@inproceedings{Ullman2009,
	Abstract = {Everyday social interactions are heavily influenced by our snap judgments about others' goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is `helping' or `hindering' another's attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent's behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative.},
	Author = {T. Ullman and C. L. Baker and O. Macindoe and O. Evans and N. D. Goodman and J. B. Tenenbaum},
	Booktitle = {Advances in Neural Information Processing Systems 22},
	Title = {Help or hinder: Bayesian models of social goal inference.},
	Website = {http://web.mit.edu/tomeru/www/papers/nips2010.pdf},
	Year = {2009}}

@inproceedings{FrankEtAl2009-Cogsci,
	Abstract = {Language does not directly code facts about the world. Instead, speakers and listeners rely on shared assumptions to allow them to communicate more efficiently. Writers like Grice and Sperber & Wilson have proposed that communication is assumed to be "informative" or "relevant," but the predictions of these accounts are often informal or post-hoc. Here we propose a formal analogue to these accounts: that communicators choose what they want to say by how informative it would be about their intended meaning. We derive quantitative predictions about how this assumption would be used in language production and learning and test these predictions via two experiments. This work takes a first step towards formalizing the pragmatic assumptions necessary for effective communication in under-constrained, real-world situations.},
	Author = {M. C. Frank and N. D. Goodman and P. Lai and J. B. Tenenbaum},
	Booktitle = {Proceedings of the 31st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Informative communication in word production and word learning},
	Website = {papers/FrankEtAl2009-Cogsci.pdf},
	Year = {2009}}

@techreport{McAllesterEtAl2008-CSAIL,
	Abstract = {We consider three desiderata for a language combining logic and probability: logical expressivity, random-world semantics, and the existence of a useful syntactic condition for probabilistic independence. Achieving these three desiderata simultaneously is nontrivial. Expressivity can be achieved by using a formalism similar to a programming language, but standard approaches to combining programming languages with probabilities sacrifice random-world semantics. Naive approaches to restoring random-world semantics undermine syntactic independence criteria. Our main result is a syntactic independence criterion that holds for a broad class of highly expressive logics under random-world semantics. We explore various examples including Bayesian networks, probabilistic context-free grammars, and an example from Mendelian genetics. Our independence criterion supports a case-factor inference technique that reproduces both variable elimination for BNs and the inside algorithm for PCFGs.},
	Author = {D. McAllester and B. Milch and N. D. Goodman},
	Institution = {Massachusetts Institute of Technology},
	Number = {MIT-CSAIL-TR-2008-025},
	Title = {Random-World Semantics and Syntactic Independence for Expressive Languages},
	Website = {papers/McAllesterEtAl2008-CSAIL.pdf},
	Year = {2008}}

@article{Schulz2008,
	Abstract = {Given minimal evidence about novel objects, children might learn only relationships among the specific entities, or they might make a more abstract inference, positing classes of entities and the relations that hold among those classes. Here we show that preschoolers (mean: 57 months) can use sparse data about perceptually unique objects to infer abstract physical causal laws. These newly inferred abstract laws were robust to potentially anomalous evidence; in the face of apparent counter-evidence, children (correctly) posited the existence of an unobserved object rather than revise the abstract laws. This suggests that children's ability to learn robust, abstract principles does not depend on extensive prior experience but can occur rapidly, on-line, and in tandem with inferences about specific relations.},
	Author = {Laura E. Schulz and Noah D. Goodman and Joshua B. Tenenbaum and Adrianna C. Jenkins},
	Doi = {n.2008.07.017},
	Journal = {Cognition},
	Month = {Nov},
	Number = {2},
	Owner = {noah},
	Pages = {211--223},
	Pii = {S0010-0277(08)00179-0},
	Pmid = {18930186},
	Timestamp = {2009.01.09},
	Title = {Going beyond the evidence: abstract laws and preschoolers' responses to anomalous data.},
	Volume = {109},
	Website = {http://eccl.mit.edu/papers/SchulzGoodman_etal.pdf},
	Year = {2008},
	Bdsk-Url-1 = {//dx.doi.org/2008.07.017}}

@misc{roy2008stochastic,
	Abstract = {We use Church, a Turing-universal language for stochastic generative processes and the probability distributions they induce, to study and extend several objects in nonparametric Bayesian statistics. We connect exchangeability and de Finetti measures with notions of purity and closures from functional programming. We exploit delayed evaluation to provide finite, machine-executable representations for various nonparametric Bayesian objects. We relate common uses of the Dirichlet process to a stochastic generalization of memoization, and use this abstraction to compactly describe and extend several nonparametric models. Finally, we briefly discuss issues of computability and inference.},
	Author = {Roy, DM and Mansinghka, VK and Goodman, ND and Tenenbaum, JB},
	Journal = {Nonparametric Bayesian Workshop, Int. Conf. on Machine Learning},
	Pages = {26},
	Title = {A stochastic programming perspective on nonparametric Bayes},
	Volume = {22},
	Website = {http://danroy.org/papers/RoyManGooTen-ICMLNPB-2008.pdf},
	Year = {2008}}

@inproceedings{KatzEtAl2008-Cogsci,
	Abstract = {Semantic knowledge is often expressed in the form of intuitive theories, which organize, predict and explain our observations of the world. How are these powerful knowledge structures represented and acquired? We present a framework, logical dimensionality reduction, that treats theories as compressive probabilistic models, attempting to express observed data as a sample from the logical consequences of the theory's underlying laws and a small number of core facts. By performing Bayesian learning and inference on these models we combine important features of more familiar connectionist and symbolic approaches to semantic cognition: an ability to handle graded, uncertain inferences, together with systematicity and compositionality that support appropriate inferences from sparse observations in novel contexts.},
	Author = {Katz, Y. and Goodman, N. D. and Kersting, K. and Kemp, C. and Tenenbaum, J. B.},
	Journal = {Proceedings of the Thirtieth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Modeling Semantic Cognition as Logical Dimensionality Reduction},
	Website = {papers/KatzEtAl2008-Cogsci.pdf},
	Year = {2008}}

@inbook{GoodmanEtAl2008-Chapter,
	Abstract = {Rational analysis attempts to explain aspects of human cognition as an adaptive response to the environment (Marr, 1982; Anderson, 1990; Chater, Tenenbaum, & Yuille, 2006). The dominant approach to rational analysis today takes an ecologically reasonable specification of a problem facing an organism, given in statistical terms, then seeks an optimal solution, usually using Bayesian methods. This approach has proven very successful in cognitive science; it has predicted perceptual phenomena (Geisler & Kersten, 2002; Feldman, 2001), illuminated puzzling effects in reasoning (Chater & Oaksford, 1999; Griffiths & Tenenbaum, 2006), and, especially, explained how human learning can succeed despite sparse input and endemic uncertainty (Tenenbaum, 1999; Tenenbaum & Griffiths, 2001). However, there were earlier notions of the "rational" analysis of cognition that emphasized very different ideas. One of the central ideas behind logical and computational approaches, which previously dominated notions of rationality, is that meaning can be captured in the structure of representations, but that compositional semantics are needed for these representations to provide a coherent account of thought. In this chapter we attempt to reconcile the modern approach to rational analysis with some aspects of this older, logico-computational approach. We do this via a model---offered as an extended example---of human concept learning. In the current chapter we are primarily concerned with formal aspects of this approach; in other work (Goodman, Tenenbaum, Feldman, & Griffiths, in press) we more carefully study a variant of this model as a psychological model of human concept learning.},
	Author = {Noah D. Goodman and Joshua B. Tenenbaum and Thomas L. Griffiths and Jacob Feldman},
	Booktitle = {The probabilistic mind: Prospects for rational models of cognition},
	Editor = {Michael Oaksford and Nick Chater},
	Publisher = {Oxford University Press},
	Title = {Compositionality in rational analysis: Grammar-based induction for concept learning},
	Website = {papers/GoodmanEtAl2008-Chapter.pdf},
	Year = {2008}}

@inproceedings{MayrhoferEtAl2008-Cogsci,
	Abstract = {Previous research has cast doubt on whether the Markov condition is a default assumption of human causal reasoning---as causal Bayes net approaches suggest. Human subjects often seem to violate the Markov condition in common-cause reasoning tasks. While this might be treated as evidence that humans are inefficient causal reasoners, we propose that the underlying human intuitions reflect abstract causal knowledge that is sensitive to a great deal of contextual information--- knowledge of the "causal background". In this paper, we introduce a hierarchical Bayesian model of causal background knowledge which explains Markov violations and makes additional, more fine-grained predictions on the basis of causally relevant category membership. We confirm these predictions using an experimental paradigm which extends that used in previous studies of "Markov violation."},
	Author = {Mayrhofer, R. and Goodman, N. D. and Waldmann, M. R. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of the Thirtieth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Structured Correlation from the Causal Background},
	Website = {papers/MayrhoferEtAl2008-Cogsci.pdf},
	Year = {2008}}

@inproceedings{BakerEtAl2008-Cogsci,
	Abstract = {Everyday human interaction relies on making inferences about social goals: goals that an intentional agent adopts in relation to another agent, such as "chasing", "fleeing", "approaching", "avoiding", "helping" or "hindering". We present a computational model of social goal inference that takes as input observations of multiple agents moving in some environmental context. The model infers a social goal for each agent that is most likely to have given rise to that agent's observed actions, under an intuitive theory that expects agents to act approximately rationally. We provide evidence for our theory-based approach over a simpler bottom-up motion cue-based approach in a behavioral experiment designed to distinguish the two accounts.},
	Author = {Baker, C. L. and Goodman, N. D. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of Thirtieth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Theory-based Social Goal Inference},
	Website = {papers/BakerEtAl2008-Cogsci.pdf},
	Year = {2008}}

@inproceedings{KempEtAl2008-Cogsci,
	Abstract = {Everyday knowledge about living things, physical objects and the beliefs and desires of other people appears to be organized into sophisticated systems that are often called intuitive theories. Two long term goals for psychological research are to understand how these theories are mentally represented, and how they are acquired. We argue that the language of thought hypothesis can help to address both questions. First, compositional languages can capture the content of intuitive theories. Second, any compositional language will generate an account of theory learning which predicts that theories with short descriptions tend to be preferred. We describe a computational framework that captures both ideas, and compare its predictions to behavioral data from a simple theory learning task.},
	Author = {Kemp, C. and Goodman, N. D. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of Thirtieth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Theory acquisition and the language of thought},
	Website = {papers/KempEtAl2008-Cogsci.pdf},
	Year = {2008}}

@inproceedings{PiantadosiEtAl2008-Cogsci,
	Abstract = {We present an unsupervised, cross-situational Bayesian learning model for the acquisition of compositional semantics. We show that the model acquires the correct grammar for a toy version of English using a psychologically-plausible amount of data, over a wide range of possible learning environments. By assuming that speakers typically produce sentences which are true in the world, the model learns the semantic representation of content and function words, using only positive evidence in the form of sentences and world contexts. We argue that the model can adequately solve both the problem of referential uncertainty and the subset problem in this domain, and show that the model makes mistakes analogous to those made by children. Keywords: Compositional semantics; language acquisition; Bayesian learning; Combinatory Categorial Grammar.},
	Author = {Piantadosi, S. T. and Goodman, N. D. and Ellis, B. A. and Tenenbaum, J. B.},
	Booktitle = {Proceedings of Thirtieth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {A Bayesian Model of the Acquisition of Compositional Semantics},
	Website = {papers/PiantadosiEtAl2008-Cogsci.pdf},
	Year = {2008}}

@inproceedings{ShaftoGoodman2008-Cogsci,
	Abstract = {Much of learning and reasoning occurs in pedagogical situations -- situations in which teachers choose examples with the goal of having a learner infer the concept the teacher has in mind. In this paper, we present a model of teaching and learning in pedagogical settings which predicts what examples teachers should choose and what learners should infer given a teachers' examples. We present two experiments using a novel experimental paradigm called the rectangle game. The first experiment compares people's inferences to qualitative model predictions. The second experiment tests people in a situation where pedagogical sampling is not appropriate, ruling out alternative explanations, and suggesting that people use contextappropriate sampling assumptions. We conclude by discussing connections to broader work in inductive reasoning and cognitive development, and outline areas of future work.},
	Author = {Patrick Shafto and Noah D. Goodman},
	Booktitle = {Proceedings of the Thirtieth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Teaching Games: Statistical Sampling Assumptions for Learning in Pedagogical Situations},
	Website = {papers/ShaftoGoodman2008-Cogsci.pdf},
	Year = {2008}}

@inproceedings{GoodmanEtAl2008-UncertaintyInArtificialIntelligence,
	Abstract = {Formal languages for probabilistic modeling enable re-use, modularity, and descriptive clarity, and can foster generic inference techniques. We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite PCFGs, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques.},
	Author = {N. D. Goodman and V. K. Mansinghka and D. M. Roy and K. Bonawitz and J. B. Tenenbaum},
	Journal = {Uncertainty in Artificial Intelligence},
	Title = {Church: a language for generative models},
	Website = {papers/GoodmanEtAl2008-UncertaintyInArtificialIntelligence.pdf},
	Year = {2008}}

@article{GoodmanTenenbaum2008-CognitiveScience,
	Abstract = {We propose a new model of human concept learning that provides a rational analysis of learning feature-based concepts. This model is built upon Bayesian inference for a grammatically structured hypothesis space -- a concept language of logical rules. We compare the model predictions to human generalization judgments in several well-known category learning experiments, and find good agreement for both average and individual participants generalizations. We further investigate judgments for a broad set of seven-feature concepts -- a more natural setting in several ways -- and again find that the model explains human performance.},
	Author = {Noah D. Goodman and Joshua B. Tenenbaum and Jacob Feldman and Thomas L. Griffiths},
	Journal = {Cognitive Science},
	Number = {1},
	Pages = {108---154},
	Title = {A Rational Analysis of Rule-based Concept Learning},
	Volume = {32},
	Website = {papers/GoodmanTenenbaum2008-CognitiveScience.pdf},
	Year = {2008}}

@inproceedings{KempEtAl2007-NIPS,
	Abstract = {Much of human knowledge is organized into sophisticated systems that are often called intuitive theories. We propose that intuitive theories are mentally represented in a logical language, and that the subjective complexity of a theory is determined by the length of its representation in this language. This complexity measure helps to explain how theories are learned from relational data, and how they support inductive inferences about unobserved relations. We describe two experiments that test our approach, and show that it provides a better account of human learning and reasoning than an approach developed by Goodman [1].},
	Author = {Kemp, Charles and Goodman, Noah D and Tenenbaum, Joshua B},
	Booktitle = {Advances in Neural Information Processing Systems},
	Title = {Learning and using relational theories.},
	Website = {papers/KempEtAl2007-NIPS.pdf},
	Year = {2007}}

@inproceedings{KempEtAl2007-Cogsci,
	Abstract = {Causal inferences about sparsely observed objects are often supported by causal schemata, or systems of abstract causal knowledge. We present a hierarchical Bayesian framework that learns simple causal schemata given only raw data as in- put. Given a set of objects and observations of causal events in- volving some of these objects, our framework simultaneously discovers the causal type of each object, the causal powers of these types, the characteristic features of these types, and the characteristic interactions between these types. Previous behavioral studies confirm that humans are able to discover causal schemata, and we show that our framework accounts for data collected by Lien and Cheng and Shanks and Darby.},
	Annote = {<b>[Winner of the 2007 Cognitive Science Society computational modeling prize for Higher-level Cognition.]</b>},
	Author = {Charles Kemp and Noah D. Goodman and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Twenty-ninth Annual Meeting of the {C}ognitive {S}cience {S}ociety},
	Title = {Learning causal schemata},
	Website = {papers/KempEtAl2007-Cogsci.pdf},
	Year = {2007}}

@inproceedings{GoodmanEtAl2007-Cogsci,
	Abstract = {We address the problem of learning grounded causal models: systems of concepts that are connected by causal relations and explicitly grounded in perception. We present a Bayesian framework for learning these models -- both a causal Bayesian network structure over variables and the consequential region of each variable in perceptual space -- from dynamic perceptual evidence. Using a novel experimental paradigm we show that humans are able to learn grounded causal models, and that the Bayesian model accounts well for human performance.},
	Annote = {<b>[Winner of the 1007 Cognitive Science Society computational modeling prize for Perception and Action.]</b>},
	Author = {Noah D. Goodman and Vikash Mansinghka and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Twenty-Ninth Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Learning grounded causal models},
	Website = {papers/GoodmanEtAl2007-Cogsci.pdf},
	Year = {2007}}

@inproceedings{FrankEtAl2007-NIPS,
	Abstract = {For infants, early word learning is a chicken-and-egg problem. One way to learn a word is to observe that it co-occurs with a particular referent across different situations. Another way is to use the social context of an utterance to infer the intended referent of a word. Here we present a Bayesian model of cross-situational word learning, and an extension of this model that also learns which social cues are relevant to determining reference. We test our model on a small corpus of mother-infant interaction and find it performs better than competing models. Finally, we show that our model accounts for experimental phenomena including mutual exclusivity, fast-mapping, and generalization from social cues.},
	Author = {Frank, M. C. and Goodman, N. D. and Tenenbaum, J. B.},
	Journal = {Advances in Neural Information Processing Systems},
	Publisher = {MIT Press},
	Title = {A bayesian framework for crosssituational word-learning},
	Volume = {20},
	Website = {papers/FrankEtAl2007-NIPS.pdf},
	Year = {2007}}

@inproceedings{GoodmanEtAl2006-Cogsci,
	Abstract = {We propose a rational analysis of children's false belief reasoning. Our analysis realizes a continuous, evidencedriven transition between two causal Bayesian models of false belief. Both models support prediction and explanation; however, one model is less complex while the other has greater explanatory resources. Because of this explanatory asymmetry, unexpected outcomes weigh more heavily against the simpler model. We test this account empirically by showing children the standard outcome of the false belief task and a novel "psychic" outcome. As expected, we find children whose explanations and predictions are consistent with each model, and an interaction between prediction and explanation. Critically, we find unexpected outcomes only induce children to move from predictions consistent with the simpler model to those consistent with the more complex one, never the reverse.},
	Author = {Noah D. Goodman and Chris L. Baker and Elizabeth Baraff-Bonawitz and Vikash K. Mansinghka and Alison Gopnik and Henry Wellman and Laura Schulz and Joshua B. Tenenbaum},
	Booktitle = {Proceedings of the Twenty-Eight Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Title = {Intuitive theories of mind: a rational approach to false belief},
	Website = {papers/GoodmanEtAl2006-Cogsci.pdf},
	Year = {2006}}
